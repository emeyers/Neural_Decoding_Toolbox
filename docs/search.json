[
  {
    "objectID": "downloads/additional-code/libsvm/index.html",
    "href": "downloads/additional-code/libsvm/index.html",
    "title": "LIBSVM",
    "section": "",
    "text": "LIBSVM is an external Support Vector Machine software package that is used by the Libsvm_CL classifier object. In order use the Libsvm_CL classifier, the LIBSVM software package must be installed to work with Matlab. Below are instructions giving a few different ways to get the LIBSVM working with the libsvm_CL classifier object.\n\nUse a precompiled version of LIBSVM\n\nWe have created a precompiled version of LIBSVM that should work with 32 bit (.mexw32) and 64 bit (.mexw64) windows operating systems, and with 64 bit Linux (.mexa64). To use the precompiled LIBSVM code, download the zip the files by clicking the link below, upzip the files, and put the directory libsvm-3.11/ in the ndt.1.0.0/external_libraries/ directory. Run the add_ndt_paths_and_init_rand_generator helper function to add the code to Matlab’s path and it should work.\nDownload precompiled LIBSVM 3.11\n\nDownload and compile LIBSVM on your system\n\nIf you are running an operating system not supported (or the above precompiled version of LIBSVM is not working), then you can compile LIBSVM into a MEX file on your own computer using the following instructions.\n\n\nDownload either the zip file or the tar.gz file.\n\n\nUnzip the compressed files and put the directory libsvm-3.1.x/ into the directory ndt.1.0.0/external_libraries/.\n\n\nCompile the libsvm mex files. To do this, in Matlab go to the directory external_libraries/libsvm-3.1.x/matlab/’ and type the command  &gt;&gt; make.\n\n\nRename the compiled svmtrain mex file, to svmtrain2 (e.g., rename svmtrain.mexa64 to svmtrain2.mexa64). Note that depending on the operating system used, the extension of the svmtrain mex file will be slightly different (e.g., if you are using a 32 bit version of Windows, the file will be named svmtrain.w32 and one should rename it svmtrain2.w32). The reason that svmtrain needs to be renamed is because the matlab bioinformatics toolbox has a function that is also named svmtrain, thus creating a naming conflict with LIBSVM.\n\n\n\nSuppressing LIBSVM command line output\n\nIf you compile LIBSVM yourself, a lot of output strings will printed to the Matlab command window when LIBSVM is run. This output can be suppressed by taking the following steps.\n\n\nOpen the file svm.cpp, and change the line\n\n#if 1\nstatic void info(const char *fmt, ...)\n\nto:\n#if 0\nstatic void info(const char *fmt, ...)\n\n\nOpen the file matlab/svmpredict.c and change (comment out) the following lines:\n\n        mexPrintf(\"Accuracy = %g%% (%d/%d) (classification)\\n\", \n              (double)correct/total*100,correct,total);\n\nto:\n     // mexPrintf(\"Accuracy = %g%% (%d/%d) (classification)\\n\",\n     //       (double)correct/total*100,correct,total);\n\n\nRecompile the mex file and rename it to svmtrain2 as described in steps 3 and 4 above.",
    "crumbs": [
      "Downloads",
      "Additional code",
      "LIBSVM"
    ]
  },
  {
    "objectID": "downloads/additional-code/meg-additional-decoding-functions/index.html",
    "href": "downloads/additional-code/meg-additional-decoding-functions/index.html",
    "title": "MEG/EEG decoding functions",
    "section": "",
    "text": "Additional code useful for decoding MEG/EEG data can be  downloaded here.\nPlease see the MEG/EEG decoding tutorial to learn how to use this code.",
    "crumbs": [
      "Downloads",
      "Additional code",
      "MEG/EEG decoding functions"
    ]
  },
  {
    "objectID": "downloads/download-the-toolbox/old-versions-of-the-toolbox/index.html",
    "href": "downloads/download-the-toolbox/old-versions-of-the-toolbox/index.html",
    "title": "Old versions of the toolbox",
    "section": "",
    "text": "The current version of the Neural Decoding Toolbox (version 1.0.4) and be downloaded here:\nDownload older version 1.0.4\n\nBelow are links to download older versions of the toolbox. In general we highly recommend using the latest version of the toolbox. However in order to see what has changed and to possibly replicate older results, we maintain links below to older versions of the toolbox. A list of the changes that have been made in different version of the toolbox can be found here.\nDownload older version 1.0.0\nDownload older version 1.0.2",
    "crumbs": [
      "Downloads",
      "Download the toolbox",
      "Old versions of the toolbox"
    ]
  },
  {
    "objectID": "downloads/download-the-toolbox/changelog/index.html",
    "href": "downloads/download-the-toolbox/changelog/index.html",
    "title": "Changelog",
    "section": "",
    "text": "Below is a list of what has changed in different versions of the toolbox\n\n1.0.2 (Tue, Jul 9 2013)\n\n\nFixes\n\n\n\nbasic_DS.m: fixed a bug that would cause an error to occur when shuffling the labels from simultaneously recorded data.\n\n\n\nEnhancements\n\n\n\nplot_standard_results_object.m: the constructor so can now take a [num_results_to_plot x num_time_bins] matrix that has all the decoding results precomputed rather than just a cell array with the names of standard results files. The field plot_obj.errorbar_file_names can now also be a [num_results_to_plot x num_time_bins] matrix of precomputed errorbars.\n\n\n\nplot_standard_results_TCT_object.m: the constructor can now take a [num_training_times x num_test_times] matrix with precompiled decoding results and this object will display a TCT plot with these precompiled results.",
    "crumbs": [
      "Downloads",
      "Download the toolbox",
      "Changelog"
    ]
  },
  {
    "objectID": "downloads/datasets/isik-26-letter-meg-dataset/index.html",
    "href": "downloads/datasets/isik-26-letter-meg-dataset/index.html",
    "title": "Isik 26 letter MEG dataset",
    "section": "",
    "text": "The Isik 26 letter MEG dataset was collected by Leyla Isik in Tommy Poggio’s lab and the MEG Lab at the McGovern Institute at MIT. The data was used in Figure 2b of the paper: The dynamics of invariant object recognition in the human visual system J. Neurophys 2014. \nThe data consists of 306 channel (comprised of 102 magentometers, and 204 planar gradiometers) MEG recordings from an Elekta Neuromag Triux Scanner. One subject was shown 26 black, upper-case letters, on a white background, while their neural response was recorded in the MEG scanner. Each letter was presented approximately 50 times. The data is in raster-format, and each trial consists of 233 ms of baseline data where the subject viewed a fixation cross, followed by 50 ms of data when the subject viewed the image of one letter, and 417 ms of data when they again viewed a fixation cross.\nThe data is available in two formats - the raw MEG files output by the scanner (.fif format) and preprocessed data in raster format. The raw data download also includes a file with raster labels indicating which stimulus was shown in each trial.\n\nDownload the Isik 26 letter MEG dataset in raster format    Note these datasets are large (2.5GB)\n\n\nDownload the Isik 26 letter MEG dataset in .fif format",
    "crumbs": [
      "Downloads",
      "Datasets",
      "Isik 26 letter MEG dataset"
    ]
  },
  {
    "objectID": "downloads/datasets/zhang-desimone-7-object-dataset/index.html",
    "href": "downloads/datasets/zhang-desimone-7-object-dataset/index.html",
    "title": "Zhang Desimone 7 object dataset",
    "section": "",
    "text": "The Zhang-Desimone 7 object dataset was collected by Ying Zhang in Bob Desimone’s lab in the McGovern Institute at MIT. The data was used in the supplemental figures in the paper Object decoding with attention in inferior temporal cortex, PNAS, 2011.\nThe data consists of single unit recordings from the 132 neurons in inferior temporal cortex (IT). The recordings were made while a monkey viewed 7 different objects that were presented at three different locations (the monkey was also shown images that consisted of three objects shown simultaneously and had to perform an attention task, however the dataset compiled here only consists of trials when single objects were shown). Each object was presented approximately 20 times at each of the three locations. The data is in raster-format, and each trial consists of 500 ms of baseline data where a monkey viewed a fixation dot, and 500 ms of data when a monkey viewed one of the 7 different images.\n\nDownload the Zhang-Desimone 7 object dataset",
    "crumbs": [
      "Downloads",
      "Datasets",
      "Zhang Desimone 7 object dataset"
    ]
  },
  {
    "objectID": "downloads/datasets/index.html",
    "href": "downloads/datasets/index.html",
    "title": "Datasets",
    "section": "",
    "text": "Zhang-Desimone 7 Object dataset\n\nIsik 26 Letter MEG dataset\n\n\nQi-Constantinidis Pre and Post Training dataset\n\n\nFreiwald Tsao Face Views AM dataset",
    "crumbs": [
      "Downloads",
      "Datasets"
    ]
  },
  {
    "objectID": "tutorials/additional-tutorial-resources/index.html",
    "href": "tutorials/additional-tutorial-resources/index.html",
    "title": "Additional tutorial resources",
    "section": "",
    "text": "Center for Brains, Minds and Machines tutorial",
    "crumbs": [
      "Tutorials",
      "Additional tutorial resources"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Below are some tutorials that show you how to use the Neural Decoding Toolbox. The introduction tutorial is a simple tutorial that explains the basics of how to decoding simple variables using the Neural Decoding Toolbox and should be read first. Once one has gone through the basic tutorial one can either try the generalization analysis tutorial to see how one can use the Neural Decoding Toolbox to test whether neural activity is invariant to transformations of experimental conditions, or one can get started using your own data by following the getting started with your own data tutorial.\n\n\nIntroduction tutorial on how to use the toolbox\n\n\n\n\nGeneralization tutorial that shows how to test whether neural representations are invariant to particular stimulus transformations\n\n\n\n\nA tutorial showing how to get started with your own data",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials/p-value-tutorial/index.html",
    "href": "tutorials/p-value-tutorial/index.html",
    "title": "P-value tutorial",
    "section": "",
    "text": "This tutorial shows how to run a permutation test to get p-values that can be used to evaluate when your decoding results are above chance. This tutorial assumes you are already familiar with the steps needed to do a basic decoding analysis as described in the basic tutorial\n\nPermutation tests\nWhen running a decoding analysis we often want to know if a particular decoding accuracy is higher than what we would expect by chance. This can be phrased as a hypothesis test where the null hypothesis (H0) is that the decoding results are compatible with a chance model of the classifier randomly guessing, and the alternative hypothesis (HA) is that they are higher than would be expected from chance model. We call our (probability) model of decoding accuracies if the classier was guessing, a null distribution , and a p-value is the probability of getting a random decoding accuracy as high or higher than the one we actually got from this null distribution.\n\nIn a permutation test, each statistic in the null distribution is created by running the decoding analysis with the labels randomly shuffled. Shuffling the labels breaks the relationship between the experimental conditions that occurred and thus gives us a decoding accuracy statistic that is consistent with the null hypothesis. To get a full null distribution, we repeat the procedure of shuffling the labels and running the decoding analysis many times.\nThe Neural Decoding Toolbox has built in functionality that can make it easy to create these null distributions and get p-values. In particular, what needs to be done is to set a flag in the data source to randomly shuffle the labels and then to run the analysis several times with this flag set, saving these shuffled results in a separate file every time. The standard_plot_result_object (and p_value object) will then take these shuffled files and the real results and return p-values at each point in time. While running the decoding analysis many times with the results shuffled might seem computationally intensive, by making a few assumptions (that for all intents and purposes are true) we can greatly speed up this computation. The rest of this tutorial walks you through the steps to do this analysis with the Neural Decoding Toolbox.\n\n\nGetting decoding results\nIn order to assess when your decoding results are above chance, you (obviously) first need to run a decoding analysis to get the regular decoding results. In this tutorial we will follow the outline of the introduction tutorial and try to decoding which of 7 different objects in Zhang-Desimone 7 object data set  was shown to a monkey. We will assume that we already have the data in binned-format in a file called ‘Zhang_Desimone_7objects_raster_data/’ (for more information about binning data see the basic tutorial). Let’s start by creating a directory to store our results called results/ and a directory within this directory called shuff/results and load the Neural Decoding Toolbox functions.\n\n% Two directories to store our results\nmkdir results\nmkdir results/shuff/\n\n% add the path to the Neural Decoding Toolbox\ntoolbox_directory_name = '../../ndt.1.0.4/'  \naddpath(toolbox_directory_name) \nadd_ndt_paths_and_init_rand_generator\n\nNow that we have directories to store our result and have loaded the NDT, let’s create a function called run_basic_decoding_initial() which will run the decoding analysis and save the results to the results/ directory. The code below shows this initial decoding function and is very similar to the code used in the Introduction Tutorial. The only real difference is that we set the cross-validator flag test_only_at_training_times equal to 1, which causes the decoding to only be trained and tested at the same time point - i.e., when this flag is set a full temporal generalization analysis will not be run which will greatly speed the run time of the code (at the cost of not being able to create TCT plots).\n\nfunction run_basic_decoding()\n\n% the following parameters are hard coded but could be input arguments to this function\nspecific_binned_labels_names = 'stimulus_ID';  % use object identity labels to decode which object was shown \nnum_cv_splits = 20;     % use 20 cross-validation splits \nbinned_data_file_name = 'Binned_Zhang_Desimone_7object_data_150ms_bins_50ms_sampled.mat'; % use the data that was previously binned \n\n% the name of where to save the results\nsave_file_name = 'results/Zhang_Desimone_basic_7object_results';\n\n\n% create the basic objects needed for decoding\nds = basic_DS(binned_data_file_name, specific_binned_labels_names,  num_cv_splits); % create the basic datasource object\nthe_feature_preprocessors{1} = zscore_normalize_FP;  % create a feature preprocess that z-score normalizes each feature\nthe_classifier = max_correlation_coefficient_CL;  % select a classifier\nthe_cross_validator = standard_resample_CV(ds, the_classifier, the_feature_preprocessors);  \n\n\n% we will also greatly speed up the run-time of the analysis by not creating a full TCT matrix \n% (i.e., we are only training and testing the classifier on the same time bin)\nthe_cross_validator.test_only_at_training_times = 1;  \n\n\n% run the decoding analysis and save the results\nDECODING_RESULTS = the_cross_validator.run_cv_decoding; \n\n% save the results\nsave(save_file_name, 'DECODING_RESULTS');\n\nend\n\nIf we were to run this code it would create it will create a file called Zhang_Desimone_basic_7object_results in the directory results/ that contains the basic decoding results. We will make a few modifications to this code below which will allow us to create to run a permutation test by creating a null distribution from which we can calculate p-values.\n\n\nShuffled results\nNow that we’ve created a basic function to run our decoding analyses, we can make a few modifications to this function to run create shuffled results for our the null distribution of our permutation test. In particular there are three modifications that we will need to make. The first modification we need to make is to change a flag in the data source so that the labels are shuffled prior to running the decoding analysis. By shuffling the labels we break the relationship between the data and the experimental conditions (labels) and thus the decoding accuracy we get is consistent with what we would expect by chance. We make this\n\nif shuff_num &gt; 0\n\n    % randomly shuffled the labels before running\n    ds.randomly_shuffle_labels_before_running = 1;  \n\n    % create the cross validator as before\n    the_cross_validator = standard_resample_CV(ds, the_classifier, the_feature_preprocessors);  \n    \n    % save the results with the appropriate name to the shuff_results/ directory\n    save_file_name = ['results/shuff_results/Zhang_Desimone_basic_7object_results_shuff_run_' num2str(shuff_num, '%03d')];\n    \nend\n\nWe add this code to our original file at the line before we create the cross-validator so that the data-source with the shuffle flag set to 1 will occur before we pass the data source to the cross-validator constructor method. The full code that contains these changes is shown below.\nNote: We have also added a line so that the cross-validator will only run 10 resample runs to speed up the code (this might create slightly more variable results, which if anything, will make our p-values a little more conservative). Finally we have added a few lines to repress information that is printed while the decoding analysis is run in order to reduce visual clutter.\n\n function run_basic_decoding_shuff(shuff_num)\n\n% the following parameters are hard coded but could be input arguments to this function\nspecific_binned_labels_names = 'stimulus_ID';  % use object identity labels to decode which object was shown \nnum_cv_splits = 20;     % use 20 cross-validation splits \nbinned_data_file_name = 'Binned_Zhang_Desimone_7object_data_150ms_bins_50ms_sampled.mat'; % use the data that was previously binned \n\n\n% the name of where to save the results\nsave_file_name = 'results/Zhang_Desimone_basic_7object_results';\n\n% create the basic objects needed for decoding\nds = basic_DS(binned_data_file_name, specific_binned_labels_names,  num_cv_splits); % create the basic datasource object\nthe_feature_preprocessors{1} = zscore_normalize_FP;  % create a feature preprocess that z-score normalizes each feature\nthe_classifier = max_correlation_coefficient_CL;  % select a classifier\n\n\nif shuff_num == 0\n    \n    'Currently running regular decoding results'\n    \n     % if running the regular results, create the regular cross-validator\n     the_cross_validator = standard_resample_CV(ds, the_classifier, the_feature_preprocessors);  \n     \n     % the name of where to save the results for regular (non-shuffled) decoding results as before\n     save_file_name = 'results/Zhang_Desimone_basic_7object_results';\n\nelse\n        \n    'Currently running shuffled label decoding results (data for the null distribution)'\n\n    ds.randomly_shuffle_labels_before_running = 1;  % randomly shuffled the labels before running\n\n    % create the cross validator as before\n    the_cross_validator = standard_resample_CV(ds, the_classifier, the_feature_preprocessors);  \n    \n    the_cross_validator.num_resample_runs = 10;  % only do 10 resample runs to save time\n    \n    % don't show progress to reduce visual clutter while the code is running\n    the_cross_validator.display_progress.zero_one_loss = 0;  \n    the_cross_validator.display_progress.resample_run_time = 0;\n    \n    % save the results with the appropriate name to the shuff_results/ directory\n    save_file_name = ['results/shuff_results/Zhang_Desimone_basic_7object_results_shuff_run_' num2str(shuff_num, '%03d')];\n    \nend\n\n\n\n% we will also greatly speed up the run-time of the analysis by not creating a full TCT matrix \n% (i.e., we are only training and testing the classifier on the same time bin)\nthe_cross_validator.test_only_at_training_times = 1;  \n\n\n\n% run the decoding analysis and save the results\nDECODING_RESULTS = the_cross_validator.run_cv_decoding; \n\n% save the results\nsave(save_file_name, 'DECODING_RESULTS'); \n\nend\n\nWe can then run this code 6 times in a for as shown below. The first time the code is run with i = 0, which means that our function is called as run_basic_decoding_shuff(0). This causes the regular decoding results to be run where the labels are not shuffled. In the next 5 iterations of the loop, run_basic_decoding_shuff is called with arguments valued from 1 to 5. This causes the shuffled results to be run and saved in the results/shuff_results/ directories with the names Zhang_Desimone_basic_7object_results_shuff_run_001.mat, Zhang_Desimone_basic_7object_results_shuff_run_002.mat, etc.\n\n\nfor i = 0:5\n    i\n    run_basic_decoding_shuff(i); \nend\n\nAs discussed more below, since there are 18 time bin in our binned data (i.e., in Binned_Zhang_Desimone_7object_data_150ms_bins_50ms_sampled.mat) we will ultimately have a null distribution that has 18 * 5 = 90 point in it. By default, the toolbox will call results ‘statistically significant’ if they are less than all points in the null distribution, which here is equivalent to a significance level of p-values be less than 1/90 = .0111. To have more potential precision of the smallest p-value, run the code more times with the labels shuffled (e.g, have the for loop from 1 to 10). In the sections below we also discuss how to change the significant level (alpha) when the results are plotted.\n\n\nPlotting ‘statistically significant’ times\nNow that we have all the data to create a null distribution, we can use the plot_standard_results object to load the data files we created and calculate a p-value based on how many decoding results in the null distribution are as high or higher than the real decoding results. To do this we just need to set two properties compared to the way we normally plot the results. The first property is plot_obj.p_values and tells the plot_standard_results_object where the null distribution files are. This property plot_obj.p_values needs to be set to a cell array of strings, where each string is a directory that contains shuffled results. The reason that this is a cell array is because if one is comparing different decoding results on the same figure, different directories of shuffled results are needed to create the p-values for each decoding result.\nThe second property that needs to be set is plot_obj.collapse_all_times_when_estimating_pvals = 1; This property tells us to use data from all time bins when creating the null distribution. By using data from all time points when creating the null distribution we can greatly reduce the number of times we have to rerun our code with the labels shuffled which greatly cuts down on computational time (i.e., for each shuffled run we get num_time_bin points rather than a single point). Of course this means we are assuming that the results with the labels shuffled are the same at all time points, however this is a reasonable assumption to make and empirically seems to be very close to true for the data we have looked at (if you are worried about this assumption and have enough time or computational power, simply do not set the flag plot_obj.collapse_all_times_when_estimating_pvals = 1; and just do many runs with the labels shuffled).\n\n % create a plot_standard_results_object that has the directory with the real results\nresult_names{1} = 'results/Zhang_Desimone_basic_7object_results';\nplot_obj = plot_standard_results_object(result_names);\n\n\n% create the names of directories that contain the shuffled data for creating null distributions\n% (this is a cell array so that multiple p-values are created when comparing results)\npval_dir_name{1} = 'results/shuff_results/';\nplot_obj.p_values = pval_dir_name;\n\n% use data from all time bins when creating the null distribution\nplot_obj.collapse_all_times_when_estimating_pvals = 1;\n\n% plot the results as usual\nplot_obj.plot_results;\nend\n\nThe solid strip at the bottom of the figure shows the times when the decoding results are above what we consider chance, as defined by the significance level (alpha) that can be set through the property plot_obj.p_value_alpha_level. As a default, plot_obj.p_value_alpha_level is set to 0 (or more technically, to the smallest value the computer is capable of representing). Thus decoding results are only considered statistically significant if they are greater than all of the shuffled data in the null distribution.\nWhen p-values are plotted, the latency of when the p-values are first above chance is also added to the legend of the plot. These latencies show the ends of the time bin that is first considered to have above chance decoding results. This first ‘statistically significant’ time bin is defined as the first bin of k consecutive bins that show p-values below the specified alpha level. Using k consecutive bins helps smooth the results so that the latency won’t be thrown off by having a single spurious time when a p-value is significant by chance, but instead the p-values need to be significant for several bins in a row. By default, the number of consecutive time bins is set to k = 5. This parameter can be changed by using the latency_num_consecutive_sig_bins property in the pvalue_object. In the future we plan to update this tutorial to explain how to do this (and how to use the pvalue_object more generally).",
    "crumbs": [
      "Tutorials",
      "P-value tutorial"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Neural Decoding Toolbox",
    "section": "",
    "text": "The Neural Decoding Toolbox\n\nThe Neural Decoding Toolbox is a MATLAB toolbox that makes it easy to apply decoding analyses to neural data.\n\nDownload the toolbox\nView the tutorials and documentation to learn how to use the toolbox\nTo learn more about neural decoding please see this book chapter and check out this video\nIf you prefer to analyze data in R, please check out the NeuroDecodeR package\n\n\nIf you use the toolbox in a publication, please cite: Meyers, E. (2013). The Neural Decoding Toolbox. Frontiers in Neuroinformatics, 7:8"
  },
  {
    "objectID": "about/publications/index.html",
    "href": "about/publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "If you use the Neural Decoding Toolbox in a publication, please cite the paper below which describes the design of the toolbox and how to use it:\nMeyers, E. (2013)  The Neural Decoding Toolbox. Frontiers in Neuroinformatics, 7:8.\n\nFor a list of publications that have used the Neural Decoding Toolbox, please see this Google Scholar page\n\n\n The following are some early publications used Neural Decoding Toolbox (or beta versions of it)\n\nRikhye R, Gilra A, Halassa M (2018).  Thalamic regulation of switching between cortical representations enables cognitive flexibility.  Nature Neuroscience 21:1753-1763.\nAlizadeh AM, Van Dromme I, Janssen P (2018).  Single-cell responses to three-dimensional structure in a functionally defined patch in macaque area TEO.  Journal of Neurophysiology 120:2906-2818.\nFornaciai M, Park J (2018).  Attractive Serial Dependence in the Absence of an Explicit Task.  Psychol Sci 29(3):437–446.\nFornaciai M, Park J (2018). Early Numerosity Encoding in Visual Cortex Is Not Sufficient for the Representation of Numerical Magnitude.  J Cogn Neurosci 30(12):1788–1802.\nTingley D, Buzsáki G (2018). Transformation of a Spatial Map across the Hippocampal-Lateral Septal Circuit.  Neuron.\nBoutona S, Chambona V, Tyranda R, Guggisberge S, Seecke M, Karkarf S, van de Villeg D, Girauda A (2018). Focal versus distributed temporal cortex activity for speech sound category assignment.  Proceedings of the National Academy of Sciences.\nKumar S., Popivanov ID, and Vogel R (2017). Transformation of Visual Representations Across Ventral Stream Body-selective Patches.  Cerebral Cortex.\nMeyers E, Liang A, Katsuki F, and Constantinidis C (2017). Differential Processing of Isolated Object and Multi-item Pop-Out Displays in LIP and PFC.  Cerebral Cortex.\nAlizadeh AM, Van Dromme I, Verhoef BE, and Janssen P(2017). Caudal Intraparietal Sulcus and three-dimensional vision: A combined functional magnetic resonance imaging and single-cell study.NeuroImage.\nSchmitt LI, Wimmer R, Nakajima M, Happ M, Mofakham S and Halassa M (2017). Thalamic amplification of cortical connectivity sustains attentional control.  Nature.\nKumar S, Kaposvari P, and Vogels R (2017). Encoding of Predictable and Unpredictable Stimuli by Inferior Temporal Cortical Neurons Journal of Cognitive Neuroscience.\nKamiński J, Sullivan S, Chung J, Ross I, Mamelak A and Rutishauser U (2017). Persistently active neurons in human medial frontal and medial temporal lobe support working memory Nature Neuroscience.\nStalnaker T, Berg B, Aujla N, Schoenbaum G (2016). Cholinergic Interneurons Use Orbitofrontal Input to Track Beliefs about Current State. Journal of Neuroscience.\nRutishauser U, Ye S, Koroma M, Tudusciuc O, Ross I, Chung J, Mamelak A (2015). Representation of retrieval confidence by single neurons in the human medial temporal lobe. Nature Neuroscience.\nMeyers E, Borzello M, Freiwald W, Tsao D (2015). Intelligent Information Loss: The Coding of Facial Identity, Head Pose, and Non-Face Information in the Macaque Face Patch System. Journal of Neuroscience, 35(18):7069-81.\nIsik, L., Meyers, E., Leibo, J., Poggio, T. (2014).  The dynamics of invariant object recognition in the human visual system  Journal of Neurophysiology, 111:91–102.\nMeyers, E., Qi, X.L., Constantinidis C. (2012).  Incorporation of new information into prefrontal cortical activity after learning working memory tasks. Proceedings of the National Academy of Sciences, 109:4651-4656.\nZhang Y.,, Meyers, E., Bichot, N., Serre, T., Poggio, T., and Desimone, R. (2011). Object decoding with attention in inferior temporal cortex. Proceedings of the National Academy of Sciences, 108:8850-8855. * These authors contributed equally.\nMeyers, E., Freedman, D., Kreiman, G., Miller, E., Poggio T. (2008). Dynamic Population Coding of Category Information in Inferior Temporal and Prefrontal Cortex. Journal of Neurophysiology, 100:1407-1419."
  },
  {
    "objectID": "toolbox-design/classifiers/libsvm_cl/index.html",
    "href": "toolbox-design/classifiers/libsvm_cl/index.html",
    "title": "libsvm_cl",
    "section": "",
    "text": "This classifier object uses the LIBSVM software package to implement a support vector machine. A support vector machine (SVM) is a classifier that learns a function f that minimizes the hinge loss between predictions made on the training data, while also applying a penalty for more complex f (the penalty is based on the norm of f in a reproducing kernel Hilbert space). The SVM has a parameter C that controls the trade off between the empirical loss (i.e., a smaller prediction error on the training set), and the complexity of the f. SVMs can use different kernels to create nonlinear decision boundaries.\n\nSVMs are designed to work on binary classification problems, so in order to support multi-class classification, we use two different methods. The first multi-class method is called ‘all-pairs’ and works by training separate classifiers for all pairs of labels (i.e., if there are 100 different classes then nchoosek(100, 2) = 4950 different classifiers are trained). Testing the classifier in all-pairs involves having all classifiers classify the test point, and then the class label is given to the class the was chosen most often by the binary classifiers (in the case of a tie in the number of classes that won a contest the class label is randomly chosen). The decision values for all-pairs are the number of contests won by each class (for each test point).\n\n\nThe second multi-class method is called ‘one-vs-all’ classification and works by training one classifier for each class (thus if there are 100 classes there will be 100 classifiers) using data from one class as the positive labels, and data from all the other classes as the negative labels. A test point is then run through these different classifiers and the class that has the largest SVM prediction value is returned as the predicted label (i.e., SVMs create a function f(x) = y, and the class label is usually given as sign(y), however here we are comparing the actual y values to determine the label). The decision values here are the f(x) = y values returned by the SVM. Our limited tests have found all pairs is faster and gives slightly more accurate results so it is the default (although the decision values might be considered more crude).\n\n\nNote: To use this classifier LIBSVM must be downloaded and setup to work with Matlab. Instructions on how to setup LIBSVM can be found here.\n\n\nProperties and Methods\n\n\ncl = libsvm_CL\n\n\nBasic constructor\n\n\ncl = train(cl, XTr, YTr)\n\n\nLearns a classification function that is a linear combination of the kernel functions that use the data as parameters. The learned function tries to reduce the empirical error (error on the training set) and the complexity of the learned function.\n\n\n[predicted_labels decision_values] = test(cl, XTe)\n\n\nPredicts the class of each test point (XTe) based on the function learned from the training data (by taking the sign of the learned function applied to each test point).\n\nThe following properties can be set to change this classifier’s behavior (here we are assuming that svm = libsvm_CL):\n\nsvm.C: scalar (default svm.C = 1)\n\n\nThis is the inverse of the regularization constant (1/regularization-constant) that determines the trade-off between the fit to the training data and the amount of regularization/simplicity of the learned function. A larger value of C means more emphasis on a better fit to the training data.\n\n\nsvm.kernel: ‘linear’, ‘polynomial’/‘poly’, or ‘gaussian’/‘rbf’ (default ‘linear’).\n\n\nThe type of kernel used which controls the type of functions that classifier is built from.\n\n\nIf svm.kernel = ‘polynomial’, then the following options must be set:\n\n\n\nsvm.poly_degree: scalar (no default value)\n\nThe degree of the of the polynomial.\n\n\n\nsvm.poly_offset: scalar (default svm.poly_offset = 0)\n\nAllows one to include a constant in the polynomial kernal.\n\n\n\n\nIf If svm.kernel = ‘gaussian’, then the following options must be set:\n\n\n\n\n\nsvm.gaussian_gamma (no default value)\n\nThis controls the fall off of the radial basis function kernel (larger values mean a slower fall off).\n\n\n\nsum.multiclass_classificaion_scheme (default value is ‘all_pairs’)\n\n\nThis field can be set to ‘all_pairs’ or ‘one_vs_all’ and determines if a one-vs-all or an all-pairs multi-class classification scheme is used, as described above (for binary problems both all-pairs and one-vs-all will return the same predicted labels, although the decision values will differ).\n\n\nsvm.additional_libsvm_options (default svm.additional_libsvm_options = ’’)\n\n\nThis allows one to have additional control of the classifier’s behavior by using a string that is in LIBSVM format. For more details see: http://www.csie.ntu.edu.tw/~cjlin/libsvm/",
    "crumbs": [
      "Toolbox Design",
      "Classifiers",
      "libsvm_cl"
    ]
  },
  {
    "objectID": "toolbox-design/classifiers/poisson_naive_bayes_cl/index.html",
    "href": "toolbox-design/classifiers/poisson_naive_bayes_cl/index.html",
    "title": "poisson_naive_bayes_cl",
    "section": "",
    "text": "This classifier object implements a Poisson Naive Bayes classifier. The classifier works by learning the expected number of occurrences (denoted lambda) for each feature and each class by taking the average of the training data over all trials (separately for each feature and each class). To evaluate whether a given test point belongs to class i, the log of the likelihood function is calculated using the lambda values as parameters of Poisson distributions (i.e., there is a separate Poisson distribution for each feature, that is based on the lambda value for that feature). The overall likelihood value is calculated by multiplying the probabilities for each neuron together (i.e,. Naive Bayes classifiers assume that each feature is independent), or equivalently, adding the log of the probabilities for each feature together. The class with the highest likelihood value is chosen as the predicted label, and the decision values are the log likelihood values.\n\nNote: This classifier assumes that all features values are integers. There is a flag in the constructor of basic_DS and generalization_DS that can convert firing rates (the default value created by the create_binned_data_from_raster_data function) to spike counts (or the helper function load_binned_data_and_convert_firing_rates_to_spike_counts can be used directly). Also, it should be noted that if the estimate rate parameter (lambda) for any feature is 0 (i.e., if the mean of a feature in the training data is 0), then this 0 lambda estimate will be replaced by the value 1/(num_training_points_in_class_k +1); i.e., we will assume that there is one more training point in which an event occurred. This prevents errors if an event occurred on a test point but the estimated lambda was 0.\n\n\nMethods\n\n\ncl = poisson_naive_bayes_CL\n\n\nBasic constructor\n\n\ncl = train(cl, XTr, YTr)\n\n\nLearns the mean occurrence values (lambdas) for each feature and class, and returns the trained classifier.\n\n\n[predicted_labels decision_values] = test(cl, XTe)\n\n\nPredicts the class of each test point (XTe) calculating the log likelihoods of the test data (XTe) for each class based on assuming that the features are independent and Poisson distributed.",
    "crumbs": [
      "Toolbox Design",
      "Classifiers",
      "poisson_naive_bayes_cl"
    ]
  },
  {
    "objectID": "toolbox-design/feature-preprocessors/select_or_exclude_top_k_features_fp/index.html",
    "href": "toolbox-design/feature-preprocessors/select_or_exclude_top_k_features_fp/index.html",
    "title": "select_or_exclude_top_k_features_fp",
    "section": "",
    "text": "This feature preprocessor object applies an ANOVA to the training data to find the p-value of all features. It then either uses the top k features with the smallest p-values, or it removes the features with the smallest k p-values. Additionally, this function can be used to remove the top k p-values and then use only the following j next smallest p-values (for example, this can be useful if one is interested in comparing the performance using the most selective 10 neurons to using the next most selective neurons, etc.).\n\nProperties and Methods\n\nThe following properties can be set to control the behavior of this feature preprocessor (note that either num_features_to_exclude num_features_to_use must be set) \n\nfp = select_or_exclude_top_k_features_FP\n\n\nBasic constructor\n\n\nnum_features_to_exclude\n\n\nThis property sets the number of features with the smallest p-values to exclude. This method or set_num_features_to_use (or both) must be set prior to using the object to normalize data.\n\n\nnum_features_to_use\n\n\nThis property sets the number of features with the smallest p-values to use. This property or num_features_to_exclude (or both) must be set prior to using the object to normalize data.\n\n\nfp = save_extra_info (default = 0)\n\n\nIf this property is set to 1, the p-values for all features will be saved and returned along with the classifier results. It should be noted that setting this value to 1 will greatly increase the size of the saved results file.\n\nThe following methods are used by the cross-validator algorithm to apply feature preprocessing to the data:\n\n[fp XTr_preprocesed] = set_properties_with_training_data(fp, XTr, YTr)\n\n\nCalculates the p-values for each feature by applying an ANOVA to the training data (XTr, YTr). The method then returns the training data with either only the top k most selective neurons, or with the top k features removed (or with the top k features removed and only the next j features used, if both set_num_features_to_use and set_num_features_to_remove have been set).\n\n\nX_preprocessed = preprocess_test_data(fp, X_data)\n\n\nRemoves or only uses the top k selective neurons (as determined using the training data). This method is usually applied by the cross-validator to the test data.\n\n\ncurrent_FP_info_to_save = get_current_info_to_save(fp)\n\n\nReturns the p-values from ANOVAs applied to each feature in the structure current_preprocessing_information_to_save.the_p_values_org_order. These values will be saved the cross-validator algorithm.",
    "crumbs": [
      "Toolbox Design",
      "Feature-preprocessors",
      "select_or_exclude_top_k_features_fp"
    ]
  },
  {
    "objectID": "toolbox-design/feature-preprocessors/index.html",
    "href": "toolbox-design/feature-preprocessors/index.html",
    "title": "Feature-preprocessors",
    "section": "",
    "text": "Feature preprocessors (FP) learn a set of parameters from the training data and modify both the training and the test data based on these parameters (prior to the data being sent to the classifier).\nThe Neural Decoding Toolbox comes with following feature preprocessor objects:",
    "crumbs": [
      "Toolbox Design",
      "Feature-preprocessors"
    ]
  },
  {
    "objectID": "toolbox-design/feature-preprocessors/index.html#methods-that-must-be-implemented",
    "href": "toolbox-design/feature-preprocessors/index.html#methods-that-must-be-implemented",
    "title": "Feature-preprocessors",
    "section": "Methods that must be implemented",
    "text": "Methods that must be implemented\nThe features preprocessor objects must only use the training data to learn the preprocessing parameters in order to prevent contamination between the training and test data which could bias the results. Feature preprocessors must implement the following three methods:\n\n[fp XTr_preprocessed] = fp.set_properties_with_training_data(XTr, YTr)\n\n\nThis method takes the training data (XTr) and the training labels(YTr) and learns parameters from them. It also applies preprocessing to the training data and returns the modified training data in the variable XTr_preprocessed.\n\n\nX_preprocessed = fp.preprocess_test_data(X_data)\n\n\nThe method takes in test data X_data, and applies preprocessing to it based on parameters that have previously been learned using the training data. The resulting preprocessed test data is returned in X_preprocessed.\n\n\ncurrent_FP_info_to_save = fp.get_current_info_to_save\n\n\nThis method signals to the cross-validator object to save particular information from the feature-preprocessor. If this method returns an empty matrix, then no additional information about the preprocessing will be saved.",
    "crumbs": [
      "Toolbox Design",
      "Feature-preprocessors"
    ]
  },
  {
    "objectID": "toolbox-design/index.html",
    "href": "toolbox-design/index.html",
    "title": "Toolbox Design",
    "section": "",
    "text": "The Neural Decoding Toolbox has a modular architecture that is designed around four abstract classes/(objects). This design allow one to easily add new functionality to the toolbox while reusing other components.  The four abstract classes/(objects) are:\n\n\nDatasources (DS) which generate training and test splits of the data.\n\n\nFeature preprocessors (FP) which learn parameters from the training data, and then preprocessess the the training and test data before sending the data to the classifier.\n\n\nClassifiers (CL) which learn models from the training data and then make predictions on the test data.\n\n\nCross-validators (CV) which take in a datasource, preprocessors and a classifier, and train and test the classifier to produce an estimate of the decoding accuracy.\n\n\nThese four classes are considered abstract because there are specific methods that each classes must implement. For example, all members of the classifier (CL) class must have the methods ‘train’, and ‘test’ (although they can have additional methods as well). The benefit of having specific interfaces for these abstract classes is that one can easily create new classes that implement these interfaces and these new classes will work with the rest of the Neural Decoding Toolbox.  Please click the links above to learn about what methods are required from each class.\nThe toolbox also comes with a few tools that consist of useful functions and objects for running decoding experiments and plotting results.\nIn order to use the toolbox, your data must be in one of the specified data formats, which are explained in more detail here.",
    "crumbs": [
      "Toolbox Design"
    ]
  },
  {
    "objectID": "toolbox-design/data-formats/binned-format/index.html",
    "href": "toolbox-design/data-formats/binned-format/index.html",
    "title": "binned-format",
    "section": "",
    "text": "Binned-format data contains data from multiple sites (e.g., data from many neurons, LFP channels, etc.). Data that is in binned-format is very similar to data that is in raster-format except that it contains information from multiple sites and often contains the information at a coarser resolution; for example, binned data would typically contain firing rates over some time interval sampled at a lower rate, as opposed to raster-format data that would typically contain individual spikes sampled at a higher rate. Binned-format data is typically created from raster-format data using the function create_time_averaged_binned_data_from_raster_data which converts a directory of raster-format files into a binned-format file that is useful for decoding. Like rater-format data, binned-format data also contains three variables which are: binned_data, binned_labels and binned_site_info . More details about these variables are described below and also see the documentation on raster-format for more information.\n\nbinned_data\n\n\nThe variable binned_data is a cell array, where each element of the array contains a [num_trials x num_time_bins] matrix, that has data from one site. In each of these matrices, the rows contain data from one trial, and each column is data from one bin. For example, if spiking data is used, then each row corresponds to a trial, and each column would correspond to the firing rate in a particular time period. If continuous data was used (e.g., LFP/MEG data, etc.), then each row would again correspond to a trial, and each column might correspond to some average of value over a particular time period, or perhaps some transform of the continuous data at a particular time period (e.g., a Fourier component, etc.). The number of elements in the binned_data cell array is equal to the number of recorded sites.\n\n\nbinned_labels\n\n\nThe variable binned_labels is a structure that contain cell arrays that have the labels for which conditions were present on each trial, i.e., binned_labels.variable_name1, is a num_sites length cell array with the labels for variable_name1 for each site, and binned_labels.variable_name1{iSite} is a [num_trials x 1] cell array of strings (or vector of numbers) that has all the labels for site ‘iSite’ for variable_name1 (number of elements for each site will vary depending on how many trials were recorded from that site).\n\n\nbinned_site_info\n\n\nThe variable binned_site_info is a structure that contains any additional information about the sites that should be saved. The structure contains cell arrays, where each entry in the array has data for one site.",
    "crumbs": [
      "Toolbox Design",
      "Data formats",
      "binned-format"
    ]
  },
  {
    "objectID": "toolbox-design/cross-validator/index.html",
    "href": "toolbox-design/cross-validator/index.html",
    "title": "Cross-validator",
    "section": "",
    "text": "Cross-validators (CV) take a classifier (CL), a datasource (DS) and optionally feature preprocessor (FP) objects, and they run a cross-validation decoding scheme by training and testing the classifier with data generated from the datasource object (and possibly fed through the feature pre-processing first).\nThe Neural Decoding Toolbox comes with following classifier objects:\nstandard_resample_CV",
    "crumbs": [
      "Toolbox Design",
      "Cross-validator"
    ]
  },
  {
    "objectID": "toolbox-design/cross-validator/index.html#methods-that-must-be-implemented",
    "href": "toolbox-design/cross-validator/index.html#methods-that-must-be-implemented",
    "title": "Cross-validator",
    "section": "Methods that must be implemented",
    "text": "Methods that must be implemented\nObjects that are cross-validators must implement the following method:\n\nDECODING_RESULTS = cv.run_cv_decoding\n\n\n\nThis method uses a datasource (DS) to generate training and test splits of the data, optionally applies feature preprocessors (FP) to the training and test data, sends the training data to a classifier (CL) which learns the relationship between the data and the labels, and then tests the classifier using the test data generated by the datasource (note that a datasource and a classifier must be set prior to running this method). This method may repeat the cross-validation decoding procedure multiple times by generating different data splits from the datasource in order to get more robust measures of the decoding accuracy.",
    "crumbs": [
      "Toolbox Design",
      "Cross-validator"
    ]
  },
  {
    "objectID": "toolbox-design/tools/plot_standard_results_tct_object/index.html",
    "href": "toolbox-design/tools/plot_standard_results_tct_object/index.html",
    "title": "plot_standard_results_tct_object",
    "section": "",
    "text": "This object allows one to display the results for training and testing at different time points in the experiment, i.e., temporal cross-training (TCT) results, for results that were created using the standard_resample_CV.run_cv_decoding method. The results are plotted in an imagesc matrix where the y-axis indicates the time the classifier was trained and the x-axis indicates the time when the classifier was tested. Additionally, the results can be displayed as a movie, with each frame showing the results for training the classifier at one particular time (indicated by a line segment at the bottom of the screen), and testing the classifier at all other times (and the results for training and testing at the same time as displayed as a black curve on the figure for reference).\n\nMethods\n\n\nplot_obj = plot_standard_results_TCT_object(‘result_file_name’)\n\n\nThe constructor. The input parameter for this constructor, result_file_name, is a string that has the name of a file that has the saved results in ‘standard format’ (i.e., results that were generated by running the standard_resample_CV.run_cv_decoding method). Alternatively, the constructor can take a [num_training_times x num_test_times]  matrix with precompiled decoding results and will display a TCT plot with these precompiled results. This can be useful, for example, if one wants to average several results together from different decoding experiments and then display the average results as a TCT plot, etc.\n\n\nmin_and_max_data_range = plot_obj.plot_results\n\n\nThe main method which creates a figure that displays the temporal cross-training result matrix, and also optionally displays a movie showing the decoding results for when the classifier was trained with data at different points in time. The method also returns the minimum and maximum value of that data in the variable min_and_max_data_range which is useful if one wants to plot multiple results with the same color range (see the color_result_range property below). As described below, there are several optional properties that can be set prior to calling the plot_results method that will change how the results are displayed.\n\n\nProperties\n\n\nresult_type_to_plot (default = 1). Specifies which type of decoding result should be plotted.\n\n\nIf this is set to 1, the zero-one loss results are plotted. If this is set to 2, the normalized rank results are plotted. If this is set to 3, the mean decision values are plotted. If this is set to 4, the ROC_AUC results run separately on each cross-validation split are plotted. If this is set to 5, the ROC_AUC results combined over cross-validation splits are plotted. If this is set to 6, the mutual information created from a confusion matrix that combining data from all resamples runs is plotted. If this is set to 7, mutual information created from a confusion matrix that is calculated separately and then averaged over resample runs is plotted.\n\n\nplot_time_intervals (default [])\n\n\nThis property specifies which time points in the experiment the results correspond to (i.e,. this specifies the x-axis values that the results are plotting against). If this value is set to a vector, then all results specified in result_file_names will use the same time range. Alternatively this property can be set with the following fields plot_time_intervals.bin_width, plot_time_intervals.sampling_interval, and optionally plot_time_intervals.alignment_event_time and plot_time_intervals.alignment_type, which will create a time interval that has the corresponding bin width, sampling interval and and alignment time which specified where 0 on the x-axis will be. The field alignment_type specifies whether the bins should be centered (default value of 2), whether they should be aligned to the end of the bins (value of 1) or whether they should be aligned to the start of the bin (value of 3). If this property is left empty (default value), then the plot_object attempts to use the binning parameters for each file using the saved in the DECODING_RESULTS.DS_PARAMETERS.binned_site_info.binning_parameters field which was originally created by the create_binned_data_from_raster_data function and pass through the DS and CV objects.\n\n\nplot_inds\n\n\nPlots a smaller range of the decoding results that are given by the indices listed in this vector.\n\n\nsaved_results_structure_name\n\n\nBy default this object assumes that all results were saved in a structure called DECODING_RESULTS. If the structure of the saved results has a different name, the name can be specified here and the results will be plotted using this name.\n\n\ncolor_result_range\n\n\nSpecifies the range of the colormap for showing worst to best results.\n\n\nfigure_position (default = [])\n\n\nSpecifies the position of where the figure should be plotted. This can be useful because resizing the figure after it is plotted can potentially cause errors in the tick labels. If a figure is open to a particular size prior to running this function and this property is left empty, the function will use the size of the current figure.\n\n\nTCT_figure_number (default = 1)\n\n\nThe figure number to display the TCT matrix.\n\n\nplot_colorbar (default = 1)\n\n\nIf this is set to zero, the colorbar will not be plotted.\n\n\nsignificant_event_times\n\n\nThis will cause vertical lines to be drawn at the times specified in this vector which can be used to indicate significant events that occurred during a trial.\n\n\nsignificant_training_event_times\n\n\nThis will cause horizontal lines to be drawn at the times specified in this vector which can be used to indicate significant events that occurred during particular training times in a trial. [added in NDT version 1.4]\n\n\ndecoding_result_type_name (default = [])\n\n\nSets the name of result type chosen. If this is left empty, the name will be chosen based on the type of result_type used. If displaying a movie of the results, this property will also be used for the label y-axis of the movie.\n\n\nfont_size (default = 16)\n\n\nThe font size for the axis labels.\n\n\nylabel_name (default = ‘Train time (ms)’\n\n\nThe label of the y-axis.\n\n\nxlabel_name (default = ‘Test Time (ms)’ )\n\n\nThe label of the x-axis.\n\n\nplot_training_latencies_increasing_up_the_y_axis (default = 1).\n\n\nIf this is set to 1, then the training latencies that are earlier in the trial will be plotted closer to the x-axis (i.e., the results will be equivalent to using ‘axis xy’ rather than ‘axis ij’)\n\n\ndisplay_TCT_movie (default = 1)\n\n\nIf this is set to 1, then a click through of the TCT results will be shown with a blue line indicating the classification accuracy for training the classifier at one point in time (indicated by a blue circle below the plot) and testing at all other points in time. Also plotted are the usual decoding results for training and testing at the same time (black line) as a reference. Every time a button is pressed, the blue line updates showing the results for training at the next point in time. When this property is set to 0, these movies are not shown, which is useful when you want to just created figures showing just the TCT matrix.\n\n\nmovie_figure_number (default = 2)\n\n\nThe figure number that the TCT movie will be shown in.\n\n\nProperties for TCT movie\n\nTo display movies of the TCT results, where each frame shows the results for training at t1 and testing at t2, the display_TCT_movie must be set to one (which is the default behavior of this property). If plot_obj.display_TCT_movie = 1 then the following additional properties can be set:\n\ndisplay_movie_pause_time (default = -1)\n\n\nHow long each frame of the movie should be displayed for. If this is set to a value less than one, then the frame will be shown until a key is pressed.\n\n\nmovie_figure_number (default = 2)\n\n\nThe figure number that the TCT movie will be shown in.\n\n\nmovie_save_name (default = [])\n\n\nIf this property is set to a string, the movie will be saved as an avi file using this name.\n\n\nmovie_time_period_titles (default = [])\n\n\nThis field allows the movie to display different titles at different time points (which is useful if one wants to give information about particular events such as ‘stimulus on’, ‘stimulus off’, etc.). If this field exists and if there is a subfields movie_time_period_titles.title_start_times that lists an array of times specifying when particular figure titles should be displayed, and if movie_time_period_titles.title_names  is a cell array that lists the names of the titles at particular times, then the title of the movie figure will change to display these titles at the given times. Note that movie_time_period_titles.title_start_times and movie_time_period_titles.title_names must be the same length.\n\n\nchance_level\n\n\nDraws a horizontal line at the specified chance decoding level. If this is unspecified, then the change level will be 1/num_classes for 0-1 loss results, no line will be drawn for decision values, and .5 line will be drawn for normalized rank and ROC measures, and 0 will be drawn for mutual information results.\n\n\nline_width (default = 2)\n\n\nThe line width of the plotted results.\n\n\nsliding_result_color (default = ‘b’)\n\n\nThe color of the results for the TCT movie plot. This color is also used for indicating which interval was the training time that is shown at the bottom of the plot.\n\n\nerrorbar_file_name\n\n\nIf this is set to a string that contains the name of a file that has results in ‘standard format’, it will plot errorbars using the data from the file name listed.\n\n\nerrorbar_type_to_plot (default = 1)\n\n\nIf errorbar_file_name is specified, this field specified which type of decoding variance measure should be plotted. The values for this property are:\n\n\n\nIf this is set to 1, then the standard deviations over resample runs (i.e., stdev.over_resamples) is used.\n\n\nIf this is set to 2, then the standard deviations over the mean decoding results from each cross-validation split (i.e., stdev.over_CVs) is used. The mean value over all resample trials is then plotted.\n\n\nIf this is set to 3, then the standard deviations over the mean decoding results from each cross-validation split combined together from all resample runs (i.e., over_CVs_combined_over_resamples) is used.\n\n\nIf this is set to 4, then the standard deviation of individual results (e.g., 0 1 values if the 0-1 loss is used) from each cross-validation split (all_single_CV_vals) are used. The mean value over all resample runs and CV splits is plotted.\n\n\nIf this is set to 5, then the standard deviation of individual results (e.g., 0 1 values if the 0-1 loss is used) from each CV split combined from all cross-validation splits in a single resample run (all_single_CV_vals_combined) are used. The mean value over all resample runs is plotted.\n\n\nIf this is set to 6, and ROC_AUC results are plotted, then the standard deviation of the ROC results from over the different classes (over_classes) are used. The mean results over resample runs (and CV splits for the separate_CV_ROC_results) are plotted. This value can only be set when ROC_AUC values are plotted (i.e., result_type_to_plot = 4 or 5).\n\n\n\nIt should also be noted that if separate_CV_ROC_results plotted (i.e., result_type_to_plot = 4), then errorbar_type_to_plot can only be set to values of 1, 2, or 6 and if combined_CV_ROC_results plotted (i.e., result_type_to_plot = 5), then errorbar_type_to_plot can only be set to values of 1 or 6. Also if mutual information created from a confusion matrix that combining data is plotted, then one can not plot errorbars, and if mutual information created from a confusion matrix that is calculated separately and then averaged over resamples is plotted, then errorbar_type_to_plot can only be set to values of 1.\n\n\nerrorbar_stdev_multiplication_factor (default = 1)\n\n\nWhen plotting the errorbars, the default behavior is to plot plus and minus 1 standard deviation for the standard deviation type that is specified. If errorbar_stdev_multiplication_factor is set to a value of k, then the errorbars will be plotted as mean_results plus/minus k times the standard deviation type is plotted.\n\n\nerrorbar_transparency_level (default = .2)\n\n\nSets the transparency level of the errorbars.\n\n\nerrorbar_edge_transparency_level (default = .2)\n\n\nSets the transparency level of the edges of the errorbars.",
    "crumbs": [
      "Toolbox Design",
      "Tools",
      "plot_standard_results_tct_object"
    ]
  },
  {
    "objectID": "toolbox-design/tools/index.html",
    "href": "toolbox-design/tools/index.html",
    "title": "Tools",
    "section": "",
    "text": "Tools are useful functions and objects that help with setting up decoding analyses and plot results. The Neural Decoding Toolbox comes with the following tools:\n\n\ncreate_binned_data_from_raster_data\n\nA function takes the name of a directory that contains files in raster-format and creates data that is in binned-format from these files.\n\n\n\nfind_sites_with_k_label_repetitions\n\nA function takes in labels in binned-format, and an integer k, and returns the indices for all sites (e.g. neurons) that have at least k repetitions of each experimental condition.\n\n\n\nlog_code_object\n\nA helper object saves information about the exact code that was run so that one has a record of how particular results were generated.\n\n\n\ncreate_pvalues_from_nulldist_files\n\n\n\n\nplot_standard_results_object\n\nThis object plots DECODING_RESULTS files that were created by running the standard_resample_CV.run_cv_decoding method, as a function of time.\n\n\n\nplot_standard_results_TCT_object\n\nThis object allows one to display the results for training and testing at different time points in the experiment, i.e., temporal cross-training (TCT) results, for results that were created using the standard_resample_CV.run_cv_decoding method.",
    "crumbs": [
      "Toolbox Design",
      "Tools"
    ]
  },
  {
    "objectID": "toolbox-design/tools/plot_standard_results_object/index.html",
    "href": "toolbox-design/tools/plot_standard_results_object/index.html",
    "title": "plot_standard_results_object",
    "section": "",
    "text": "This object plots DECODING_RESULTS files that were created by running the standard_resample_CV.run_cv_decoding method, as a function of time.\n\nMethods\n\n\nplot_obj = plot_standard_results_object(result_file_names)\n\n\nThe constructor. The input parameter for this constructor, result_file_name, is a cell array of strings that lists the file names of the results that will be plotted, i.e., result_file_names = {‘file_name1’, ‘file_name2’, …};. Alternatively, the constructor can take a [num_results_to_plot x num_time_bins] matrix that has all the decoding results precomputed; note, if this later option is used, then p_values can not be a cell array of strings with null distribution directory names, but must be a cell array of vectors with precomputed p-values.\n\n\nplot_obj.plot_results\n\n\nThe main method for the plot_standard_results_object which creates a figure that displays the results. There are several optional parameters that can be set prior to calling this method which will change how the results are displayed, as explained below.\n\n\nProperties\n\n\nresult_type_to_plot (default = 1). Specifies which type of decoding result should be plotted.\n\n\nIf this is set to 1, the zero-one loss results are plotted. If this is set to 2, the normalized rank results are plotted. If this is set to 3, the mean decision values are plotted. If this is set to 4, the ROC_AUC results run separately on each cross-validation split are plotted. If this is set to 5, the ROC_AUC results combined over cross-validation splits are plotted. If this is set to 6, the mutual information created from a confusion matrix that combining data from all resamples runs is plotted. If this is set to 7, mutual information created from a confusion matrix that is calculated separately and then averaged over resample runs is plotted.\n\n\nplot_time_intervals (default [])\n\n\nThis property specifies which time points in the experiment the results correspond to (i.e,. this specifies the x-axis values that the results are plotting against). If this value is set to a vector, then all results specified in result_file_names will use the same time range. Alternatively this property can be set with the following fields plot_time_intervals.bin_width, plot_time_intervals.sampling_interval, and optionally plot_time_intervals.alignment_event_time and plot_time_intervals.alignment_type, which will create a time interval that has the corresponding bin width, sampling interval and alignment time which specified where 0 on the x-axis will be. The field alignment_type specifies whether the bins should be centered (default value of 2), whether they should be aligned to the start of the bins (value of 1) or whether they should be aligned to the end of the bin (value of 3). This property can also cell array that is the same length as result_file_names, with each entry of the cell array containing a vector of specified times or a structure of binning parameters for each decoding result. If this property is left empty (default value), then the plot_object attempts to use the binning parameters for each file using the saved in the DECODING_RESULTS.DS_PARAMETERS.binned_site_info.binning_parameters field which was originally created by the create_binned_data_from_raster_data function and pass through the DS and CV objects.\n\n\nplot_inds\n\n\nPlot a smaller range of the decoding results that are given by the indices listed in this vector. This field can also be set to a cell array the that is the same length as result_file_names, with each entry of the cell array containing a vector of indices that should be plotted for each result.\n\n\nsaved_results_structure_name\n\n\nBy default this object assumes that all results were saved in a structure called DECODING_RESULTS. If the structure of the saved results has a different name, the name can be specified here and the results will be plotted using this name.\n\n\nthe_colors\n\n\nA cell array listing the colors that each result should be plotted as. There are 20 colors defined as the default colors, so if more than 20 results are being compared on the same figure, this property must be extended to include more colors.\n\n\nline_width (default = 2)\n\n\nThe line width of the plotted results.\n\n\nline_style (default = ‘-’)\n\n\nThe line style of the plotted results (i.e., can plot dots, dashes, etc.).\n\n\nfont_size (default = 16)\n\n\nThe font size for the axis labels.\n\n\nylabel_name\n\n\nThe default behavior is to display the appropriate name for the given result_type that is being plotted, but this can be set to display alternative ylabels.\n\n\nxlabel_name (default = ‘Time (ms)’ )\n\n\nThe label of the x-axis.\n\n\nlegend_names\n\n\nAllows one to set a cell array of strings corresponding to the legends for the different results\n\n\nchance_level\n\n\nDraws a horizontal line at what the chance decoding level is. If this is unspecified, then the chance level will be 1/num_classes for 0-1 loss results, no line will be drawn for decision values, and .5 line will be drawn for normalized rank and ROC AUC results, and 0 will be drawn for mutual information results.\n\n\nsignificant_event_times\n\n\nThis will cause vertical lines to be drawn at the times specified in this vector which can be used to indicate significant events that occurred during a trial.\n\n\nsignificant_event_regions\n\n\nSetting this to a cell array of 2 element vectors (i.e., significant_event_regions{1} = [t1 t2]) will cause shaded regions to be drawn between times t1 and t2.\n\n\nsignificant_event_region_alphas (default = .1)\n\n\nIf significant_event_regions is set to a cell array of 2 dimensional vectors, the shaded regions with have an alpha level of transparency set by this property. If this property is set to a vector that is the same size as the cell array, then each shaded region can have a different alpha level as determined by the entries in this vector.\n\n\nsignificant_event_region_colors (default = [0 0 0])\n\n\nIf significant_event_regions is set to a 3 dimensional vector, the shaded regions with have a color specified by this [r g b] vector. If this property is set to a cell array that is the same size as the significant_event_regions cell array, then each shaded region can have a different color as determined by the 3 dimensional vectors in each cell.\n\n\nthe_axis (default = [])\n\n\nThis has the same effect as using axis([x1 x2 y1 y2]) function but setting this first will allow significant event lines and shaded regions to be drawn to span the whole display, as opposed to if this is set later using the axis function.\n\n\np_values\n\n\nIf this is set to a cell array of numbers that is the same length as result_file_names with each vector containing p-values for each time point, then bars will be plotted at the bottom of the plot for all times that the p-values are less than the p_value_alpha_level. If this is set to a cell array of strings that contain the names of directories that have files that comprise a null distribution, the p-values will be calculated and bars will be shown at the bottom of the plot for all times that the p-values are less than p_value_alpha_level. The bars for the pvalues indicate which time points in the decoding results shown above are higher than expected by chance. Technically each bar should really be a dot indicating which result data point is above chance but for illustrative purposes we have extended them to bars (that have the width of sampling interval) so that a solid line is created for continuous times that are above chance (the downside of this is that it might give a slightly false impression of the first time when the results are above chance). For more information about how the p-values are calculated see pvalues_object.\n\n\np_value_alpha_level\n\n\nIf the p_values field is set, then bars will be shown at the bottom of the plot for all times that the p-values are less than this value.\n\n\ncollapse_all_times_when_estimating_pvals (default = 0).\n\n\nIf this is set to one, the null distributions from all time bins are combined together to create one larger total null distribution. The p-values are then calculated by comparing the actual decoding accuracy at each point in time to this larger null distribution (with this same null distribution is used for all points in time). The advantage of using this is that if the null distributions at each point in time are the same, then one can get a more precise estimate of the p-values for the same computational cost. [added in NDT version 1.4]\n\n\nadd_pvalue_latency_to_legends_alignment (default = 4)\n\n\nIf this property is set to value greater than 0, and if the p_values field has been set, then the latency when the decoding results are first above chance will be added to the legend. If this value is set to 1 it will show the beginning of the time interval when the results are first above chance, if it is set to 2 it will show the middle of the time interval when the results are first above chance, if this is set to 3 it will show the end of the time interval when the results are first above chance and if this is set to 4 (default) it will show the beginning and the end of the interval when the results are first above chance. If this property is set to 5, then the time interval that was added to make the results above chance will be displayed (i.e., the time of the first interval that is above chance will be displayed minus any time that is in the previous interval (which was the last time interval that was not above chance)).\n\n\nerrorbar_file_names\n\n\nIf this is set to a cell array of strings, it will plot errorbars using the data from the file names listed. The number of file names in this cell array must be the same as the number of file names in the result_file_names sent to the constructor of this object (i.e., each result to be plotted much have a corresponding errorbar file if errorbars are to be plotted). Alternatively, errorbar_file_names can be set to a [num_results_to_plot x num_time_bins] sized matrix that contains precomputed errorbars (and the field errorbar_type_to_plot will be ignored).\n\n\nerrorbar_type_to_plot (default = 1)\n\n\nIf errorbar_file_names are specified, this field specified which type of decoding variance measure should be plotted. The values for this property are:\n\n\n\nIf this is set to 1, then the standard deviations over resample runs (i.e., stdev.over_resamples) is used.\n\n\nIf this is set to 2, then the standard deviations over the mean decoding results from each cross-validation split (i.e., stdev.over_CVs) is used. The mean value over all resample trials is then plotted.\n\n\nIf this is set to 3, then the standard deviations over the mean decoding results from each cross-validation split combined together from all resample runs (i.e., over_CVs_combined_over_resamples) is used.\n\n\nIf this is set to 4, then the standard deviation of individual results (e.g., 0 1 values if the 0-1 loss is used) from each cross-validation split (all_single_CV_vals) are used. The mean value over all resample runs and CV splits is plotted.\n\n\nIf this is set to 5, then the standard deviation of individual results (e.g., 0 1 values if the 0-1 loss is used) from each CV split combined from all cross-validation splits in a single resample run (all_single_CV_vals_combined) are used. The mean value over all resample runs is plotted.\n\n\nIf this is set to 6, and ROC_AUC results are plotted, then the standard deviation of the ROC results from over the different classes (over_classes) are used. The mean results over resample runs (and CV splits for the separate_CV_ROC_results) are plotted. This value can only be set when ROC_AUC values are plotted (i.e., result_type_to_plot = 4 or 5).\n\n\n\nIt should also be noted that if separate_CV_ROC_results plotted (i.e., result_type_to_plot = 4), then errorbar_type_to_plot can only be set to values of 1, 2, or 6 and if combined_CV_ROC_results plotted (i.e., result_type_to_plot = 5), then errorbar_type_to_plot can only be set to values of 1 or 6. Also if mutual information created from a confusion matrix that combining data is plotted, then one can not plot errorbars, and if mutual information created from a confusion matrix that is calculated separately and then averaged over resamples is plotted, then errorbar_type_to_plot can only be set to values of 1.\n\n\nerrorbar_stdev_multiplication_factor (default = 1)\n\n\nWhen plotting the errorbars, the default behavior is to plot plus and minus 1 standard deviation for the standard deviation type that is specified. If errorbar_stdev_multiplication_factor is set to a value of k, then the errorbars will be plotted as mean_results plus/minus k times the standard deviation type is plotted.\n\n\nerrorbar_transparency_level (default = .2)\n\n\nSets the transparency level of the errorbars.\n\n\nerrorbar_edge_transparency_level (default = .2)\n\n\nSets the transparency level of the edges of the errorbars.",
    "crumbs": [
      "Toolbox Design",
      "Tools",
      "plot_standard_results_object"
    ]
  },
  {
    "objectID": "toolbox-design/tools/create_binned_data_from_raster_data/index.html",
    "href": "toolbox-design/tools/create_binned_data_from_raster_data/index.html",
    "title": "create_binned_data_from_raster_data",
    "section": "",
    "text": "This helper function takes the name of a directory that contains files in raster-format and creates data that is in binned-format from these files. The function has the following form:\n\n\n\nsaved_binned_data_file_name = create_binned_data_from_raster_data(raster_file_directory_name, save_prefix_name, bin_width, sampling_interval, [start_time], [end_time])\n\n\nThe arguments to this function are:\n\n\n\nraster_file_directory_name\n\nThe path to the directory that contains the files in raster format.\n\n\n\nsave_prefix_name\n\nThe beginning of a file name (possibly including a directory name) that specifies the name that the binned data should be saved as. Appended on to the end of this name when the binned data is saved is the bin width, step size, and possibly start and end times used.\n\n\n\nbin_width\n\nThe bin size that is averaged over when creating binned-format features. For example, if the raster files have spike times given with millisecond precision, and bin_width = 500, then the binned-format data will contain average firing rates (i.e., the spike-count rate) in 500 ms bins. If bin_width is a vector that is the same size as sampling_interval then different bin widths can be specified for different time periods (e.g., if bin_widths = [50 500, 100], then the first bin will average over 50 ms of data, the second over 500 ms of data, and the third over 100 ms of data).\n\n\n\nsampling_interval\n\nSpecifies the sampling interval between successive binned-data points. For example, if the raster files have spike times given with millisecond precision, and if sampling_interval = 50, then a binned data point will be computed at 50ms intervals. If sampling_interval is a vector, then the values in this vector will specify the start times for the bins (e.g., if sampling_interval = [1 200 500], then the first bin will start time 1, the second bin will start at time 200, and the third bin will start at time 500).\n\n\n\n\nOptional arguments:\n\n\n\nstart_time\n\nThis specifies the time to start the binning process. If this argument is not set, then the binning will start with the first data point in the raster files (i.e., the first bin will use raster_data(:, 1:bin_width), the second bin will use data from raster_data(:, (step_size+1):(step_size+bin_width)), etc.).\n\n\n\nend_time\n\nThis specifies the time when to end the binning process. If this argument is not set, then the binning will end with the last data point in the raster files.\n\n\n\n\nNote: if sampling_interval is a vector, the arguments 5 and 6 can not be set (this is because the start bin times have already been exactly specified by the sampling_interval vector).\n\nThe output of this function is to saves a file in the directory specified by the save_prefix_name (the name of the file is returned in the variable saved_binned_data_file_name). The saved file contains the three variables needed to conform to binned-format which are: binned_data which contains the data, binned_labels which contains the labels, and binned_site_info which contains the any extra information about each site. The data in the three saved variables are extracted from the raster-format files that are contained in the input directory. Additionally, additional information is saved in binned_site_info.binning_parameters that contains the bin_width, sampling interval, etc., that were used to create this binned data.\n\nExamples\n\n\nExample 1:\n\nSuppose we had a directory called my_raster_file_directory/ that contained a number of files in raster-data format from the spike times of neurons specified at 1 ms resolution. Then running:\ncreate_binned_data_from_raster_data('my_raster_file_directory/', ...\n     'my_save_dir/binned_data', 150, 50, 200, 1000)\n\nwill create a file called binned_data_150ms_bins_50ms_sampled_200start_time_1000end_time that will be saved in the directory my_save_dir/. This file will contain the average firing rate calculated over 150 ms intervals for all neurons in the directory my_raster_file_directory/. The firing rates will be sampled every 50 ms, starting 200 ms into the raster-format data and ending 1000 ms into the raster-format data.\n\n\nExample 2:\n\nSuppose we had a directory called my_raster_file_directory/ that contained a number of files in raster-data format from the spike times of neurons specified at 1 ms resolution. Then running:\ncreate_binned_data_from_raster_data('my_raster_file_directory/', ...\n     'my_save_dir/binned_data_100ms_bins_plus_2_extra', ...\n      [100 .* ones(10, 1) 250 250], [1:50:500 1 251]);\n\nwill create a file in the binned_data_100ms_bins_plus_2_extra_custom_bins_custom_sampled that will be saved in the directory my_save_dir/. This file will contain the average firing rate calculated over 100 ms intervals for all neurons in the directory my_raster_file_directory/ and will be sampled every 50 ms for the first 500 ms of data. Additionally, there will be two 250 ms bins at the end of the binned data that have the average firing rates over 1-250 ms into the trial and 251-500 ms into the trial.",
    "crumbs": [
      "Toolbox Design",
      "Tools",
      "create_binned_data_from_raster_data"
    ]
  },
  {
    "objectID": "toolbox-design/datasources/generalization_ds/index.html",
    "href": "toolbox-design/datasources/generalization_ds/index.html",
    "title": "generalization_ds",
    "section": "",
    "text": "This datasource object (DS) allows one to train a classifier on a specific set of labels, and then test the classifier on a different set of labels. This enables one to evaluate how similar neural representations are across different but related conditions (i.e., does training on one set of conditions generalization to a different but related set of conditions ). This datasource is a subclass of the handle class (i.e., it has a persistent state) and contains a basic_DS where it gets most of its functionality from.\n\nThe constructor for this datasource contains contains the same arguments as basic_DS, plus two additional arguments the_training_label_number and the_test_label_numbers i.e., the constructor has the form: ds = generalization_DS(the_data, the_labels, num_cv_splits, the_training_label_numbers, the_test_label_numbers). the_training_label_number and the_test_label_numbers are cell arrays that specify which labels should belong to which class, with the first element of these cells arrays specifying the training/test labels that should be in first class, the second element of the cell array specifies which labels belong to the second class, etc.. For example, suppose one was interested in testing position invariance, and had done an experiment in which data was recorded while 7 different objects were shown at three different locations. If the labels for the 7 objects at the first location had labels ‘obj1_loc1’, ‘obj2_loc1’, …, ‘obj7_loc1’ at the second location were ‘obj1_loc2’, ‘obj2_loc2’, …, ‘obj7_loc2’, and at the third location were ‘obj1_loc3’, ‘obj2_loc3’, …, ‘obj7_loc3’, then one could do a test of position invariance by setting the_training_label_names{1} = {‘obj1_loc1}, the_training_label_names{2} = {’obj2_loc1’}, …, the_training_label_names{7} = {‘obj7_loc1’},  and setting the_test_label_names{1} = {‘obj1_loc2’, ‘obj1_loc3’}, the_test_label_names{2} = {‘obj2_loc2’, ‘obj2_loc3’}, …, the_test_label_names{7} = {‘obj7_loc2’, ‘obj7_loc3’}. This DS object is able to test such generalization from training on one set of labels and testing on a different set of labels by remapping the training label numbers to the index number in the_training_label_names cell array, and remapping the test label numbers with the the index number into the the_test_label_names cell array.\n\n\nThere is also an additional property that can be set for this object which is: use_unique_data_in_each_CV_split (default value is 0). When this argument is set to 0, the get_data method returns the normal leave one split out training and test data sets (i.e., the training set consists of (num_cv_splits - 1) splits of the data and the test set consists of 1 split of the data).\n\n\nThe data in the training still comes from different splits as the data in the test set, thus one can have some of the same labels in the both the_training_label_names and in the_test_label_names (in fact, if ones sets the_test_label_names = the_training_label_names , then the get_data method will be the same as the basic_DS get_data  method). However, if use_unique_data_in_each_CV_split = 1, then each training and test set will consist data from only split, and thus each cross-validation run is essentially like running an independent decoding experiment. In this case the_training_label_names and the_test_label_names must not contain any of the same labels (otherwise, they would be copies of the same data which would violate the fact that the training and the test set must not have any of the same data).\n\n\nMethods\n\n\nds = generalization_DS(binned_data_name, specific_binned_label_names, num_cv_splits, the_training_label_numbers, the_test_label_numbers, load_data_as_spike_counts)\n\n\nThe constructor, which takes the following inputs:\n\n\n\nbinned_data_name\n\nA string containing the name of a file that has data in binned-format, or alternatively, a cell array of data in binned-format\n\n\n\nspecific_binned_label_name \n\nA string containing the name of specific binned labels, or alternatively, a cell array (or vector) containing the specific binned labels (e.g., binned_labels.specific_binned_labels)\n\n\n\nnum_cv_splits\n\nA number indicating how many cross-validation splits there should be\n\n\n\nthe_training_label_numbers\n\nA cell array specifying which labels should belong to which class, with the first element of this cell arrays specifying the training labels for the first class the second element of the cell array specifying which labels belong to the second class, etc.\n\n\n\nthe_test_label_numbers\n\nA cell array specifying which test labels should belong to which class, with the first element of this cell arrays specifying the test labels for the first class the second element of the cell array specifying which labels belong to the second class, etc.\n\n\n\nload_data_as_spike_counts\n\nIf this optional argument is set to an integer greater than 0, this will convert the data from firing rates (the default value saved by create_binned_data_from_raster_data function) to spike counts. This is useful when using the Poisson Naive Bayes classifier which only works on spike count data.\n\n\n\n[XTr_all_time_cv YTr_all XTe_all_time_cv YTe_all] = get_data(ds)\n\n\nThe same arguments are basic_DS but now the data and labels are based on the grouping given by the_training_label_numbers and the_test_label_numbers that are set in the constructor\n\n\n\nthe_properties = get_DS_properties(ds)\n\n\nAlso returns the properties values for the_training_label_numbers, the_test_label_numbers and use_unique_data_in_each_CV_split.\n\n\nds = set_specific_sites_to_use(ds, curr_bootstrap_sites_to_use)\n\n\nExact same functionality inherited from basic_DS\n\n\nProperties\n\nIn addition to the properties inherited from basic_DS, generalization_DS also has the following property that can be set:\n\nuse_unique_data_in_each_CV_split (default = 0).\n\n\nWhen this argument is set to 0, the get_data method returns the normal leave one split out training and test data sets (i.e., the training set consists of (num_cv_splits - 1) splits of the data and the test set consists of 1 split of the data). The data in the training still comes from different splits as the data in the test set, thus one can have some of the same labels in the both the_training_label_numbers and in the_test_label_numbers (in fact, if one has  the_test_label_numbers = the_training_label_numbers, then the get_data method will be the same as the basic_DS get_data method. However, if use_unique_data_in_each_CV_split = 1, then each training and test set will consist data from only split, and each cross-validation split will consist of unique data. In this case the_training_label_numbers and the_test_label_numbers must not contain any of the same labels (otherwise, they would be copies of the same population vector which would violate the fact that the training and the test set must not have any of the same data).\n\nThe following properties function the same way as in basic_DS (for more information see the basic_DS documentation):\n\n\ncreate_simultaneously_recorded_populations (default = 0).\n\n\nsample_sites_with_replacement (default = 0).\n\n\nnum_times_to_repeat_each_label_per_cv_split (default = 1).\n\n\nnum_resample_sites (default = -1, which means use all sites).\n\n\nsites_to_use (default = -1).\n\n\nsites_to_exclude (default = []).\n\n\ntime_periods_to_get_data_from (default = []).\n\n\nrandomly_shuffle_labels_before_running (default = 0).",
    "crumbs": [
      "Toolbox Design",
      "Datasources",
      "generalization_ds"
    ]
  },
  {
    "objectID": "toolbox-design/datasources/basic_ds/index.html",
    "href": "toolbox-design/datasources/basic_ds/index.html",
    "title": "basic_ds",
    "section": "",
    "text": "The datasource basic_DS implements the basic of a datasource (DS) object. Namely, it takes binned data and labels, and through the  get_data()  method, the object returns k leave-one-fold-out cross-validation splits of the data which can subsequently be used to train and test a classifier. The data in the population vectors is randomly selected subset of data from larger population of binned-format data that is passed to the constructor of this object. This object can create both pseudo-populations (i.e., populations vector in which the recordings were made on independent sessions but are treated as if they were recorded simultaneously) and simultaneously populations in which neurons that were recorded together always appear together in population vectors.\n\n\nNote: this object inherits from the handle class. Thus when an object from this class is created, a reference to the object is returned, and modifications to any object property immediately affect the object without having to assign the object to a new variable.\n\n\nMethods\n\n\nds = basic_DS(binned_data_name, specific_binned_label_names, num_cv_splits, load_data_as_spike_counts)\n\n\nThe constructor, which takes the following inputs:\n\n\n\nbinned_data_name\n\nA string containing the name of a file that has data in binned-format, or alternatively, a cell array of data in binned-format. Note that it is preferred to pass a string containing the name of the binned-format file since this will cause the binned_site_info to be saved by the datasource (and this information can be saved in the cross-validator in order to plot timing information automatically).\n\n\n\nspecific_binned_label_name \n\nA string containing the name of specific binned labels, or alternatively, a cell array (or vector) containing the specific binned labels (e.g., binned_labels.specific_binned_labels)\n\n\n\nnum_cv_splits\n\nA number indicating how many cross-validation splits there should be\n\n\n\nload_data_as_spike_counts\n\nIf this optional argument is set to an integer greater than 0, this will convert the data from firing rates (the default value saved by create_binned_data_from_raster_data function) to spike counts. This is useful when using the Poisson Naive Bayes classifier which only works on spike count data.\n\n\n\n\n[XTr_all_time_cv YTr_all XTe_all_time_cv YTe_all] = get_data(ds)\n\n\nThe main get_data method that needs to be implemented for all DS objects. The outputs are the standard DS variables:\n\n\n\nXTr_all_time_cv{iTime}{iCV} = [num_features x num_training_points] \n\nIs a cell array that has the training data for all times and cross-validation splits\n\n\n\nYTr_all = [num_training_point x 1] \n\nAre the training labels\n\n\n\nXTe_all_time_cv{iTime}{iCV} = [num_features x num_test_points] \n\nIs a cell array that has the test data for all times and cross-validation splits;\n\n\n\nYTe_all = [num_test_point x 1] \n\nAre the test labels\n\n\n\n\nthe_properties = get_DS_properties(ds)\n\n\nThis method returns the main property values of the datasource.\n\n\nds = set_specific_sites_to_use(ds, curr_resample_sites_to_use)\n\n\nThis method causes the get_data to use specific sites rather than choosing sites randomly. This method should really only be used by other datasources that are extending the functionality of basic_DS.\n\n\nProperties\n\nThe basic_DS also has the following properties that can be set:\n\ncreate_simultaneously_recorded_populations (default = 0)\n\n\nIf the data from all sites was recorded simultaneously, then setting this variable to 1 causes the function to return simultaneous populations rather than pseudo-populations (for this to work all sites in ‘the_data’ must have the trials in the same order). If this variable is set to 2, then the training set will be pseudo-populations and the test set will be simultaneous populations. This allows one to estimate Idiag (as described by Averbeck, Latham and Pouget in ‘Neural correlations, population coding and computation, Nature Neuroscience, May, 2006’) which is a measure that gives a sense of whether training on pseudo-populations leads to a different decision rule as compared to when training on actual simultaneous recordings.\n\n\nsample_sites_with_replacement (default = 0)\n\n\nThis variable specifies whether the sites should be sampled with replacement - i.e., if the data is sampled with replacement, then some sites will be repeated within a single population vector. This allows one to do a bootstrap estimate of variance of the results if different sites from a larger population had been selected while also ensuring that there is no overlapping data between the training and test sets.\n\n\nnum_times_to_repeat_each_label_per_cv_split (default = 1)\n\nThis variable specifies how many times each label should appear in each cross-validation split. For example, if this value is set to k, this means that there will be k population vectors from each class in each test set, and there will be k x (num_cv_splits - 1) population vectors for each class in each training set split.\n\nlabel_names_to_use (default = [] meaning all unique label names in the_labels are used)\n\n\nThis specifies which labels names (or numbers) to use, out of the unique label names that are present in the the_labels cell array. If only a subset of labels are listed, then only population vectors that have the specified labels will be returned.\n\n\nnum_resample_sites (default = -1, which means use all sites)\n\n\nThis variable specifies how many sites should be randomly selected each time the get_data method is called. For example, suppose length(the_data) = n, and num_resample_sites = k, then each time get_data is called, k of the n sites would randomly be selected to be included as features in the population vector.\n\n\nsites_to_use (default = -1, which means select features from all sites)\n\n\nThis variable allows one to only choose features from the sites listed in this vector (i.e., features will only be randomly selected from the sites listed in this vector).\n\n\nsites_to_exclude (default = [], which means do not exclude any sites)\n\n\nThis allows one to not select features from particular sites (i.e., features will NOT be selected from the sites listed in this vector).\n\n\ntime_periods_to_get_data_from (default = [], which means create one feature for all times that are present in the_data{iSite} matrix)\n\n\nThis variable can be set to a cell array that contains vectors that specify which time bins to use as features from the_data. For examples, if time_periods_to_get_data_from = {[2 3], [4 5], [10 11]} then there will be three time periods for XTr_all_time_cv and XTe_all_time_cv (i.e., length(XTr_all_time_cv) = 3), and the population vectors for the time period will have 2 x num_resample_sites features, with the population vector for the first time period having data from each resample site from times 2 and 3 in the_data{iSite} matrix, etc..\n\n\nrandomly_shuffle_labels_before_running (default = 0)\n\n\nIf this variable is set to 1, then the labels are randomly shuffled prior to the get_data method being called (thus all calls to get_data return the same randomly shuffled labels). This method is useful for creating a null distribution to test whether a decoding result is above what one would expect by chance.",
    "crumbs": [
      "Toolbox Design",
      "Datasources",
      "basic_ds"
    ]
  },
  {
    "objectID": "toolbox-design/datasources/index.html",
    "href": "toolbox-design/datasources/index.html",
    "title": "Datasources",
    "section": "",
    "text": "Datasource objects (DS) take binned data and labels as inputs and generate population vectors (which are used by a cross-validator object to train and test a classifier).\nThe Neural Decoding Toolbox comes with following datasource objects:",
    "crumbs": [
      "Toolbox Design",
      "Datasources"
    ]
  },
  {
    "objectID": "toolbox-design/datasources/index.html#methods-that-must-be-implemented",
    "href": "toolbox-design/datasources/index.html#methods-that-must-be-implemented",
    "title": "Datasources",
    "section": "Methods that must be implemented",
    "text": "Methods that must be implemented\nDatasources must implement a get_data method that returns the training and test data splits. The get_data method must have the following structure:\n\n\n[all_XTr, all_YTr, all_XTe, all_YTe, ADDITIONAL_INFO] = ds.get_data\n\n\n The outputs of this methods are: \n\n\n1.all_XTr{iTime}{iCV} = [num_features x num_training_points]\n\n\nA cell array that has the training data for all times and cross-validation splits\n\n\n2.all_YTr = [num_training_point x 1]\n\n\nThe training labels\n\n\n3.all_XTe{iTime}{iCV} = [num_features x num_test_points]\n\n\nA cell array that has the test data for all times and cross-validation splits\n\n\n4.all_YTe = [num_test_point x 1]\n\n\nThe test labels",
    "crumbs": [
      "Toolbox Design",
      "Datasources"
    ]
  },
  {
    "objectID": "toolbox-design/tools/find_sites_with_k_label_repetitions/index.html",
    "href": "toolbox-design/tools/find_sites_with_k_label_repetitions/index.html",
    "title": "find_sites_with_k_label_repetitions",
    "section": "",
    "text": "This helper function takes in labels in binned-format, and an integer k, and returns the indices for all sites (e.g. neurons) that have at least k repetitions of each experimental condition. The function has the following form:\n\n[inds_of_sites_with_at_least_k_repeats, min_num_repeats_all_sites, num_repeats_matrix label_names_used] = find_sites_with_k_label_repetitions(the_labels, k, label_names_to_use)\n\nThe arguments to this function are:\n\n\nthe_labels\n\nThe labels in binned-format that should be used (e.g., binned_labels.the_labels_to_use).\n\n\n\nk\n\nAn integer specifying that each site returned should have at least k repetitions of each condition.\n\n\n\nOptional input arguments:\n\n\nlabel_names_to_use\n\nThis specifies what label names (or numbers) to use. For example, if the_labels contains have strings consisting of ‘red’, ‘green’, ‘blue’, but you only want to know which sites have k repeats of ‘red’ and ‘green’ trials, then setting this to label_names_to_use = {‘red’, ‘green’} will accomplish this goal. If this argument is not specified, then any label that was presented to any site will be used.\n\n\n\nReturned values:\n\n\ninds_of_sites_with_at_least_k_repeats\n\nThe indices of sites that have at least k repetitions of each condition.\n\n\n\nmin_num_repeats_all_sites\n\nThis vector lists, for each site, the number of repetitions present for the label that has the minimum number of repetitions.\n\n\n\nnum_repeats_matrix\n\nA [num_sites x num_labels] matrix that specifies for each site, the number of repetitions of each condition. This variable could be useful for determining if particular conditions should be excluded based on whether a specific condition was presented only a few times to many of the sites.\n\n\n\nlabel_names_used\n\nA specifies what label names were used when counting repetitions. This variable is equal to label_names_to_use if label_names_to_use was passed as an input argument. [added in NDT version 1.4]\n\n\n\n\nExample\n\nSuppose we had an experiment in which a number of different stimuli were shown when recordings were made from a number of different sites, and this information was contained in the variable binned_labels.stimulus_ID. The following command would find all sites in which each stimulus condition was presented at least 20 times:\ninds_of_sites_with_at_least_k_repeats = find_sites_with_k_label_repetitions(binned_labels.stimulus_ID, 20)\nWhen one is first starting to analyze a new dataset, one can also use this function to assess how many times each condition has been presented to each site in order to determine how many cross-validation splits to use. Examining the variable min_num_repeats_all_sites could be useful for this purpose, or one could run the following command:\nfor k = 0:60\n    inds_of_sites_with_at_least_k_repeats = find_sites_with_k_label_repetitions(binned_labels.stimulus_ID, k);\n    num_sites_with_k_repeats(k + 1) = length(inds_of_sites_with_at_least_k_repeats);\nend\nThe variable num_sites_with_k_repeats(i) indicates how many sites have at least i - 1 repetitions, i.e., num_sites_with_k_repeats(1) gives the total number of sites, num_sites_with_k_repeats(2) gives how many sites have at least one presentation of each stimulus, etc.. Note that 2 repetitions is the minimum needed to do a decoding analyses, although to get reasonable results usually needs at least 5 repetitions of each condition.",
    "crumbs": [
      "Toolbox Design",
      "Tools",
      "find_sites_with_k_label_repetitions"
    ]
  },
  {
    "objectID": "toolbox-design/tools/pvalue_object/index.html",
    "href": "toolbox-design/tools/pvalue_object/index.html",
    "title": "pvalue_object",
    "section": "",
    "text": "This helper object calculates p-values from a file that has the decoding results in standard format, and a directory of files that has a number of ‘null distribution’ decoding results which were created by running the same decoding experiment a number of times with the labels randomly shuffled. The p-values are thus based on a permutation test which gives the probability that the real decoding results came from the null distribution. The object also has a method to calculate the latency of when the decoding results were first above chance. (This object inherits from the handle class so it maintains its state after method calls).\n\nMethods\n\n\npval_obj = pvalue_object(real_decoding_results_file_name, null_distribution_directory_name)\n\n\nThe constructor has two optional arguments that are described below. If the object is created without passing these arguments, then the fields pval_obj.real_decoding_results_file_name and pval_obj.null_distribution_directory_name must both be set before calling the pval_obj.create_pvalues_from_nulldist_files method.\n\n\n\nreal_decoding_results_file_name\n\nA string specifying the name of a file that has real decoding results. These results should be in standard results format (as created by the standard_resample_CV.run_cv_decoding method.\n\n\n\nnull_distribution_directory_name \n\nA string specifying the name of a directory that has multiple decoding result file that were run with the labels shuffled. The p-values are created by loading each file in this directory to create a null probability distribution that estimates what decoding accuracies would be expected to occur by chance.\n\n\n\n\n[p_values null_distributions PVALUE_PARAMS] = pval_obj.create_pvalues_from_nulldist_files\n\n\nThis method creates p-values from the actual decoding results and the null distribution results (i.e., p-values are based on a permutation test). More specifically, all the files in the null distribution directory are loaded to create null distributions at each point in time. At each time point, a p-value is calculated based on the proportion of decoding values in the null distribution that exceeds the real decoding result value. This method causes the properties pval_obj.p_values and pval_obj.null_distributions to be set, and also returns the following output values:\n\n\n\np_values\n\na vector containing the p-values at each point in time (i.e., the probability one would get a decoding accuracy as high as the one reported if there was no relationship between the data and the class labels).\n\n\n\nnull_distributions\n\nA [num_points_in_null_distribution x num_times] matrix containing the null distribution values at each point in time.\n\n\n\nPVALUE_PARAMS\n\nA list of parameters that were used to create the p-values (see the pval_obj.get_pvalue_parameters method below).\n\n\n\n\n[latency_time latency_ind] = pval_obj.get_latency_of_decoded_information\n\n\nThis method returns an estimate of the latency (i.e., time) when the decoding results are above chance. The pval_obj.create_pvalues_from_nulldist_files  method must be called (or the field pval_obj.p_values must be manually set) prior to running this method, and additionally latency_time_interval property must also be set. This method works by finding all p_values that are less than or equal to pval_obj.latency_alpha_significance_level, and then returning the first time in which the p-values are at or below this significance level for pval_obj.latency_num_consecutive_sig_bins .\n\n\nPVALUE_PARAMS = pval_obj.get_pvalue_parameters\n\n\nThis method returns a number of parameters that were used to calculate the p-values and get the decoding information latency. The structure, PVALUE_PARAMS , that is returned by this object has the following fields:\n\n\n\n.num_points_in_null_distribution\n\nThe number of points in the null distribution.\n\n\n\n.smallest_significance_level\n\nthe smallest possible significance level that can be used based on the number of points in the null distribution (i.e., this value is equal to 1/PVALUE_PARAMS.num_points_in_null_distribution). If the parameter pval_obj.latency_alpha_significance_level is set to 0 (default value), then the p-values should be listed as p &lt; PVALUE_PARAMS.smallest_significance_level\n\n\n\n.result_type_name\n\nA string specifying what type of results were used to create these p-values (e.g., ZERO_ONE_LOSS, etc.)\n\n\n\n.training_time_ind_to_use\n\nThe pval_obj.training_time_ind_to_use value that was used.\n\n\n\n.real_decoding_results_file_name:\n\nThe pval_obj.real_decoding_results_file_name string that was used.\n\n\n\n.saved_results_structure_name\n\nThe pval_obj.saved_results_structure_name string that was used.\n\n\n\n.null_distribution_file_prefix_name\n\nThe pval_obj.null_distribution_file_prefix_name that was used.\n\n\n\n.latency_alpha_significance_level\n\nThe pval_obj.latency_alpha_significance_level value that was used.\n\n\n\n.latency_num_consecutive_sig_bins\n\nThe pval_obj.latency_num_consecutive_sig_bins that was used.\n\n\n\n.latency_time_interval\n\nThe pval_obj.latency_time_interval that was used.\n\n\n\n\nProperties\n\nThe following properties can be set to change the behavior of this object:\n\nreal_decoding_results_file_name\n\n\nThe name of the non-shuffled decoding results that are compared to the null distribution to see when the results are above chance. This value can also be set in the constructor.\n\n\nnull_distribution_directory_name\n\n\nThe name of the directory that contains the null distribution results files. This value can also be set in the constructor.\n\n\ncollapse_all_times_when_estimating_pvals (default = 0).\n\n\nIf this is set to one, the null distributions from all time bins are combined together to create one larger total null distribution. The p-values are then calculated by comparing the actual decoding accuracy at each point in time to this larger null distribution (with this same null distribution is used for all points in time). The advantage of using this is that if the null distributions at each point in time are the same, then one can get a more precise estimate of the p-values for the same computational cost. [added in NDT version 1.4]\n\n\nthe_result_type (default = 1)\n\n\nSpecifies which type of decoding result should be used when calculating the p-values.\n\n\nIf this is set to 1, the zero-one loss results are used. If this is set to 2, the normalized rank results are used. If this is set to 3, the mean decision values are used. If this is set to 4, the ROC_AUC results run separately on each cross-validation split are used. If this is set to 5, the ROC_AUC results combined over cross-validation splits are used. If this is set to 6, the mutual information created from a confusion matrix that combining data from all resamples runs is used. If this is set to 7, mutual information created from a confusion matrix that is calculated separately and then averaged over resample runs is used.\n\n\nnull_distribution_file_prefix_name (deafult is ’’)\n\n\nA string that specifies what the beginning of the names of files in the null distribution directory is. This is useful if there are multiple types of results (e.g., from different decoding analyses) stored in the directory that has the null files but you only want the results in some of these files to be used.\n\n\ntraining_time_ind_to_use (default = -1)\n\n\nIf a full TCT matrix has been created, this specifies which row (i.e., training time bin) of the TCT matrix should be used when calculating the p-values. Setting this to a value of less than zero creates p-values when the classifier was trained and tested from the same time bin (i.e., the diagonal of the TCT plot, or equivalent vector of results if the classifier was only trained and tested at the same time).\n\n\nsaved_results_structure_name (default is ‘DECODING_RESULTS’)\n\n\nA string specifying the name of the variable that has the decoding results.\n\n\np_values (default is [])\n\n\nThese are the p-values that are usually set by calling the pval_obj.create_pvalues_from_nulldist_files method. However, one can set these values manually, and then one can use the pval_obj.get_latency_of_decoded_information method. Doing this is useful if one is getting the latency many times so that one calculate the p-values once, save them, and then load them into this object to get the latency.\n\n\nlatency_time_interval\n\n\nThis specifies which time points in the experiment the p-values correspond to, and must be set prior to calling the pval_obj.get_latency_of_decoded_information method. This property can be set to either: 1) a vector specifying which times to use, 2) a time_interval_object that one can get time_intervals from or 3) a structure with the fields latency_time_interval.bin_width, latency_time_interval.sampling_interval, and optionally latency_time_interval.alignment_event_time, latency_time_interval.start_time, and latency_time_interval.end_time which will create a time interval that has the corresponding bin widths, step sizes, zero time, start_time and end_time of the time interval.\n\n\nlatency_time_interval_alignment_type (default = [], which means show the beginning and end of the first significant time bin)\n\n\nIf latency_time_interval is set to a vector of numbers, this property will be ignored. However if latency_time_interval is set to a time_interval_object or to a structure containing latency_time_interval.bin_width and latency_time_interval.sampling_interval then this will cause the latency estimate to report either: the beginning time of the first significant time bin (latency_time_interval_alignment_type = 1), the middle time of the first significant time bin (latency_time_interval_alignment_type = 2), the end time of the first significant time bin (latency_time_interval_alignment_type = 3), the beginning and end time of the first significant time bin (atency_time_interval_alignment_type = 4, default), the time interval of data that was added to make a bin significant, relative to the previous bin which was not significant (latency_time_interval_alignment_type = 5), or use the alignment already specified in the time_interval_object or by the structure latency_time_interval.alignment_time (latency_time_interval_alignment_type = 6).\n\n\nlatency_alpha_significance_level (default = 0))\n\n\nThe significance level (alpha value) that the p-values must be less than in order to claim that the results have not occurred by chance. When calculated the latency of decoding information, all the p-values are compared to this significance level value to determine whether a time point is considered significant (and the latency is determined based on whether there are pval_obj.latency_num_consecutive_sig_bins significant bins in a row).\n\n\nlatency_num_consecutive_sig_bins (default = 5)\n\n\nThe number of consecutive time bins that must be significant in order for a specific time to be considered the time point when the results are first above chance. The reason this property is needed is because calculating p-values at many time periods introduces a high probability that one will have a small p-value even when the null hypothesis is correct (i.e., a type 1 error which is due to multiple comparisons issues that commonly affect null-hypothesis significance tests). A commonly used (ad hoc) method in neuroscience to deal with this issue is to define the latency as the time when the p-values remain significant of multiple consecutive bins, which is the method we are using here (empirically it appears to produce reasonable results).\n\n\nreal_decoding_results_lower_than_null_distribution (defult = 0)\n\n\nIf this field is set to one the pvalue_object will calculate the p-values based on the proportion of null distribution decoding results are lower than the actual real decoding result - i.e., it will test the probability that the real decoding result would have been that low by chance. This is useful as a sanity check to make sure the decoding procedure it working (and to test for anti-learning). [added in NDT version 1.4]",
    "crumbs": [
      "Toolbox Design",
      "Tools",
      "pvalue_object"
    ]
  },
  {
    "objectID": "toolbox-design/tools/log_code_object/index.html",
    "href": "toolbox-design/tools/log_code_object/index.html",
    "title": "log_code_object",
    "section": "",
    "text": "This helper object saves information about the exact code that was run so that one has a record of how particular results were generated. This object gives several different methods for saving code in different files/memory configurations. The results are in a structure that has the name and path of the file being recorded, the text in the body of the file, the time when the file was last modified and the time when the log_code_object recorded the file This method inherits from the handle class so all logged files are persistently kept in the logged_code_structure as new files are added.\n\nMethods\n\n\nlog_code_obj = log_code_object\n\n\nThe constructor\n\n\nlog_code_obj.log_current_file\n\n\nThis method logs the file that the called this method.\n\n\nlog_code_obj.log_specfic_files(file_names)\n\n\nIf file_name is a string, this method logs the file that is in file_names, or if files_names is a cell array of strings, this method logs all the files that are in the cell array.\n\n\nlog_code_obj.log_files_in_directory(directory_name, file_pattern)\n\n\nThis method logs all the files in the directory directory_name that have the pattern listed in file_pattern. For example, to log all the m-files in the current directory one would use the methods: log_code_obj.log_files_in_directory(‘./’, ’*.m’)\n\n\nlog_functions_in_memory(log_code_obj, directory_name)\n\n\nThis method logs all the functions that are in memory from a particular directory given in directory_name. This is particularly useful if there are a large number of files in a given directory but you only want to save the ones that were used by your code (see examples below).\n\nTo get the logged information one can call the following method (or just access the field logged_code_structure):\n\nlogged_code_structure = log_code_obj.return_logged_code_structure\n\n\nThe number of elements in logged_code_structure is equal to the number of files logged, and the structure has the following fields:\n\n\n\n.code_filenames\n\nThe name of the file that has been recorded.\n\n\n\n.code_filepath\n\nThe full path to the directory where the file has been recorded.\n\n\n\n.code_body\n\nThe text that is in the file.\n\n\n\n.last_modified_time\n\nThe time when the file was last modified.\n\n\n\n.logged_time\n\nThe time the file was logged (useful in case the file was changed after being logged).\n\n\n\nExample\n\nWe recommend calling the method log_code_obj.log_current_file in the beginning of any code you have the specifies what decoding object/parameters are used in a data analysis (that way if you change the code while the decoding analysis is running the original parameters will be recorded). If one wants to log which functions in the Neural Decoding Toolbox were used, one can then call the log_code_obj.log_functions_in_memory(path_to_toolbox) method to save all the functions that were used. For example:\n% start of decoding scripts\n\nlog_code_obj = log_code_object; \nlog_code_obj.log_current_file;   % log the script that is currently being run\n          \n% create the decoding objects, run the decoding analysis with DECODING_RESULTS = cv.run_cv_decoding;\n  \nlog_code_obj.log_functions_in_memory(path_to_toolbox);  % log the functions that were used in the NDT\n\ndecoding_code_log = log_code_obj.logged_code_structure;\nsave('my_results', 'DECODING_RESULTS', 'decoding_code_log');\n      \n`&lt;/pre&gt;",
    "crumbs": [
      "Toolbox Design",
      "Tools",
      "log_code_object"
    ]
  },
  {
    "objectID": "toolbox-design/cross-validator/standard_resample_cv/index.html",
    "href": "toolbox-design/cross-validator/standard_resample_cv/index.html",
    "title": "standard_resample_cv",
    "section": "",
    "text": "This is a cross-validator object that run a cross-validation procedure multiple times (as specified by the num_resample_runs property) and returns decoding results calculated over the resample runs and cross-validation splits. This cross-validator calculates a decoding measures based on the zero-one loss function (i.e., classification accuracy and mutual information based on the confusion matrix). If a classifier is used that returns decision values, then one can also get results based on these decision values, normalized rank results, and results based on the area under ROC curves that are calculated separately for each class. More information about the methods, different properties that can be set, and different results that can be returned as described below.\n\nMethods\n\n\ncv = standard_resample_CV(the_datasource, the_classifier, the_feature_preprocessors)\n\n\nThis constructor sets the datasource and classifier objects that will be used in the cross-validation procedure. One can also optionally pass a cell array of feature preprocessor objects that will be applied (in the order they are listed in the cell array) prior to the data being passed to the classifier.\n\n\nDECODING_RESULTS = run_cv_decoding(cv)\n\n\nThis method runs a cross-validation decoding procedure multiple times (in a bootstrap-like manner) using different splits of the data that are generated by the datasource, and returns the results in the DECODING_RESULTS structure.\n\n\nProperties\n\n\nThe following properties can be set to change the behavior of this CV object.\n\n\nnum_resample_runs (default = 50)\n\n\nSets the minimum number of resample runs that the decoding procedure will be run.\n\n\ntest_only_at_training_times (default = 0)\n\n\nIf this is set to zero then the classifier will be trained and testing at all times, which will create a “temporal cross-training” matrix of results that have the decoding accuracies for training at time 1 and testing at time 2. This allows one to test if information is contained in a dynamic population code (e.g., see Meyers et al., J. Neurophysiology, 2008). If this properties is set to 1, then the results will only be run for training and testing at the same time, which can cause the run_cv_decoding method to potentially run much faster, particularly if it classifier is slow.\n\n\nstop_resample_runs_only_when_specfic_results_have_converged (default = 0)\n\n\nSetting the different fields in this structure causes the resample runs to continue beyond those specified by num_resample_runs if the results have not converged to a stable estimate. stop_resample_runs_only_when_specfic_results_have_converged has the following fields that can be set which control the resample run stopping criteria for different types of results:\n\n\n.zero_one_loss_results\n\n\ncontrols whether the zero one loss results have converged\n\n\n.normalized_rank_results\n\n\ncontrols whether the normalized rank results have converged\n\n\n.decision_values\n\n\ncontrols whether the decision value results have converged; .combined_CV_ROC_results: controls whether the combined CV ROC results have converged\n\n\n.separate_CV_ROC_results\n\n\ncontrols whether the separate CV ROC results have converged. By default all these fields are set to empty meaning that no converge stopping criteria are set by default.\n\n\nSetting any of these fields to a particular value causes the run_resample_cv method to keep running the resample run loop until the given mean result (over resample runs) changes by less than the specified value (over all training and test time periods) when any one resample run is left out. This can be useful for speeding up the run-time when the decoding results have converged to a stable value. For example, one could set num_resample_runs to a lower number (say 20) and then setting .zero_one_loss_results to a smallish value (say .1), which might cause fewer than the default value of 50 resample runs to be executed while getting results that are almost as accurate - i.e., there would at most be a .1 change in the decoding accuracy (and any point in time) if the most sensitive resample run was left out. If any of these fields are set, there will still be a minimum number of resample runs executed that is specified by the num_resample_runs property, and then there will be additional resample runs to be executed until the desired convergence level is achieved. There is also an additional field .stop_criteria_is_absolute_result_value (default = 1), which specifies whether the value set should be taken as an absolute change in the decoding accuracy - e.g., the actual zero-one decoding result values should change by less than 1 when any resample run is left out. If this field is set to 0, then the values specified are given as a percentage change that should not occur if any resample run is left out relative to the maximum of the mean decoding accuracy - i.e., a value of 1 would mean that the results of leaving the ith resample run out, should not chance by more than 1% at that time point relative to the maximum decoding accuracy achieved (since the scale of a plot is determined relative to the maximum decoding accuracy this shows how much variance there is in the results on a plot due to not using more resample runs). [added in NDT version 1.4]\n\n\nIf a classifier is used that returns decision values along with zero-one loss results, then additional decoding accuracy results can be calculated and returned by setting the following options:\n\n\nsave_results\n\n\n.normalized_rank (default = 1)\n\n\nCalcuates and returns normalized rank results.\n\n\n.decision_values (default = 1)\n\n\nReturns the decision values generated by the classifier to make a classification.\n\n\n.extended_decision_values (default is 0)\n\n\nIf this property is set to 1 then all the decision values for the correct class are saved. If this property is set to 2, then all decision values from all classes are saved (doing this can potentially take up a lot of memory/disk space)\n\n\n.ROC_AUC (default = 1)\n\n\nReturns area under ROC curve results. These results are calculated separately for each class k, with the test decision values from class k being the positive examples, and all other decision values being the negative examples. The results can be calculated separately on the test points on each CV split, (save_results.ROC_AUC = 3) or from combining all decision values over all cross-validation splits (save_results.ROC_AUC = 2). If save_results.ROC_AUC = 1, then both separate CV results, and combined CV results will be saved.\n\n\n.mutual_information (default = 1)\n\n\nReturns estimates of mutual information calculated from the confusion matrix.\n\n\nA number of parameters can be set to create confusion matrices for the results. A confusion matrix is a matrix in which the columns express the true class label and the rows express the predicted label. For example, if column 2, and row 3 of a confusion matrix had a value of 7, it would mean that there were 7 times in which class 2 was mistakenly predicted to be class 3 (summing the columns will give the total number of examples for each class, which can be used to turn the confusion matrix into a misclassification probability distribution). The following parameters can be set to create confusion matrices:\n\n\nconfunsion_matrix_params\n\n\n.create_confusion_matrix (default is 1)\n\n\nCreates and saves a zero-one loss confusion matrix.\n\n\n.save_confusion_matrix_only_train_and_test_at_same_time (default is 1)\n\n\nSaves the confusion matrix only for when training and testing was done at the same time.\n\n\n.create_all_test_points_separate_confusion_matrix (default is 0)\n\n\nCreates a confusion matrix where all test points are separate, i.e., if there are two test points from the same class on a given CV split, these two test points will be given separate column in the confusion matrix (this is useful when labels have been remapped using the generalization_DS datasource).\n\n\nThe display_progress options allow one to display the decoding result (for different result types) as the decoding procedure as it is running. The results displayed show the mean decoding results, as well as as a measure of the variability of the results, which gives a sense of whether enough resample iterations have been run so that the results have converged to a stable solution. The measure of variability of the results is calculated by computing the mean decoding results if the i-th resample run was left out (i.e., if there was one less resample run). This is done for each resample run, and the standard deviation is taken over these one-resample-left-out means. Thus this measure gives a sense of how much the results would vary if one less resample iteration was run, which gives a rough sense of whether the results have converged if one adds these to the mean results (overall these numbers should be very small). The following options allow one to display progress of different result types. The results for different types are displayed for all times, and are separated by NaNs?, (i.e,. mean_type1 stdev_type1 NaN mean_type2 stdev_type2, NaN, etc.).\n\n\ndisplay_progress\n\n\n.resample_run_time (default = 1)\n\n\nDisplays how many resample runs have completed, the amount of time the last resample run took, and an estimate of the time when the code will be done running.\n\n\n.zero_one_loss (default = 1)\n\n\nDisplay zero-one loss results.\n\n\n.normalized_rank (default = 0)\n\n\nDisplay normalized rank results.\n\n\n.decision_values (default = 0)\n\n\nDisplay decision values.\n\n\n.separate_CV_ROC_results (default = 0)\n\n\nDisplay ROC AUC results computed separately on each CV split.\n\n\n.combined_CV_ROC_results (default = 0)\n\n\nDisplay ROC AUC results combined points from all CV splits.\n\n\n.training_time_to_display_results (default = -1)\n\n\nThe training time for which to display the test result values. A value of -1 means that the displayed results will for training at the test time points (or if only one training time was used, then a value of -1 means use that time for all test times).\n\n\n.convergence_values (default = 0)\n\n\nDisplays the current resample run convergence values and convergence target value (see the explanation above of the field stop_resample_runs_only_when_specfic_results_have_converged for more information). [added in NDT version 1.4]\n\n\nReturned results\n\nThe run_cv_decoding method returns a structure called DECODING_RESULTS that contains the following properties.\n\nDECODING_RESULTS.\n\n\nFP_INFO{iFP}\n\n\nIf any of the feature pre-processing algorithms had returned information to be saved via their get_current_FP_info_to_save method, then this structure will contain the additional information that the FP algorithm wanted to be saved.\n\n\nCV_PARAMETERS\n\nThis structure contains additional information about parameters used in the decoding procedure. This structure has the following fields:\n\n.num_test_points_on_each_CV_run\n\n\nThe number of test points on each CV split.\n\n\n.num_training_points_on_each_CV_run\n\n\nThe number of training points on each CV split.\n\n\n.dimension_of_data_points\n\n\nThe dimensionality of the training/test data points.\n\n\n.unique_labels\n\n\nA vector containing the unique test labels that were used.\n\n\n.num_unique_labels\n\n\nThe number of unique labels (classes)\n\n\n.num_CV_splits\n\n\nThe number of cross-validation splits of the data.\n\n\n.num_resample_runs\n\n\nThe number of resample runs the cross-validator went through.\n\n\n.toolbox_version_number\n\n\nThe toolbox version number.\n\n\n.classifier_name\n\n\nThe class name of the classifier.\n\n\n.feature_preprocessor_names\n\n\nThe class names of the feature preprocessors.\n\n\nDS_PARAMETERS\n\nIf the datasource used has a method called get_properties, then the properties returned by this method will be saved in the structure DS_PARAMETERS.\n\nZERO_ONE_LOSS_RESULTS\n\nThis structure contains the main (zero-one loss) decoding results. The following results can be returned.\n\n.decoding_results\n\n\nA [num_resample_runs x num_CV_splits x num_training_times x num_test_times] matrix that contains all the decoding results separate for each resample run and cross-validation split.\n\n\n.mean_decoding_results\n\n\nA [num_training_times x num_test_times] matrix that contains the mean decoding results averaged over all resample runs and CV splits.\n\n\n.confusion_matrix_results\n\n\nIf the confusion matrix properties have been set (e.g., cv.confusion_matrix_params.create_confusion_matrix == 1), then this structure can contain the following fields:\n\n\n.confusion_matrix\n\n\nA [num_predicted_classes x num_actual_classes x num_training_times x num_test_times] confusion matrix specifying how often a test point j was classified as belonging to class i (where j indicates column indices, and i indicates row indices). If .save_confusion_matrix_only_train_and_test_at_same_time = 1 , then the confusion matrix is [num_predicted_classes x num_actual_classes x num_training_times] large and only contains the confusion matrices when training and testing at the same time period (which saves a lot of disk space when saving the results).\n\n\n.label_remapping\n\n\nIf the labels given to the classifier are not consecutive integers, this vector indicates how the labels have been remapped on to the columns of the confusion matrix (i.e., the first value in the vector indicates the class in the first row of the confusion matrix, etc.).\n\n\n.all_test_points_separate_confusion_matrix\n\n\nIf the create_all_test_points_separate_confusion_matrix flag is set to 1, then this variable contains a confusion matrix that is [num_test_points x num_actual_classes x num_training_times x num_test_times] large, with each test point in a given cross-validation split being given a separate row in the confusion matrix (this confusion matrix will only differ from the regular confusion matrix if there are multiple test points from the same class in a given cross-validation split). This matrix can be useful if the labels have been remapped to different classes using the generalization_DS datasource in order to see what classes were confused in the original unremapped labels.\n\n\n.stdev\n\n\nThis structure contains information about the variability of the results. This structure has the following fields:\n\n\n.all_single_CV_vals\n\n\nEach cross-validation run produces a value for each test point (i.e., 0 and 1’s, normalized ranks, or decision values). This [num_resamples, num_CV_splits, num_train, num_test] matrix contains the standard deviation over these test points for each cross-validation run.\n\n\n.all_single_CV_vals_combined\n\n\nA [num_resamples, num_train, num_test] matrix that is the same as .all_single_CV_vals but all the values from the cross-validation runs are combined together first before the standard deviation is taken (this is done separately for each resample run).\n\n\n.over_CVs\n\n\nA [num_resamples, num_train, num_test] matrix that is calculate by taking the mean decoding results in each cross-validation run (i.e., the mean of the 0, 1’s, ranks or decision values of the points in one cross-validation split) and then taking the standard deviation over these mean cross-validation split results (this is done separately for each resample run).\n\n\n.over_CVs_combined_over_resample(num_train, num_test)\n\n\nThis is the same as .over_CVs but all the values from all the resample runs are combined before the standard deviation is taken.\n\n\n.over_boostraps\n\n\nThis [num_train, num_test] matrix calculates the mean over all the mean CV values for a resample run, and then calculates the standard deviation over the different resample runs.\n\nIt should be noted that .over_CVs, .over_CVs_combined_over_resamples and .over_boostraps could have all been computed running the decoding experiment using the values in .decoding_results, but we precompute them here for convenience.\n\nFor classifiers that return decision values, this cross-validator object can return additional results in the structures NORMALIZED_RANK_RESULTS, DECISION_VALUES and ROC_AUC_RESULTS that are described below.\n\n\nNORMALIZED_RANK_RESULTS\n\nThis structure contains the normalized rank results. Normalized rank results are the results based on using the prediction values from the classifier to create an ordered list of predictions (i.e., the most likely class is x, second most likely class is y, etc.), and then assessing how far done on the list is the correct label. The results are normalized so that perfect prediction has a value of 1, chance has a value of .5 and having the last prediction be the correct one leads to a value of 0. The .NORMALIZED_RANK_RESULTS has the same .mean_decoding_results and .stdev results as .ZERO_ONE_LOSS_RESULTS but the different confusion matrix values. The confusion matrix for the normalized rank results is in the field .confusion_matrix_results.rank_confusion_matrix and contains matrix that is [num_predicted_classes x num_actual_classes x num_training_times x num_test_times] which contains values in the ith row and jth column for the average normalized rank ith predicted class when test points from the jth actual class were presented (i.e., how high up on the predicted labels list is the ith class when test points from the jth class are shown). There is also a field .confusion_matrix_results.rank_confusion_matrix_label_mapping that contains the labels that correspond to the columns of the rank confusion matrix.\n\nDECISION_VALUES\n\nThis structure contains the decision values. It has all the same fields as the ZERO_ONE_LOSS_RESULTS except that there is no confusion matrix for this result type. If save_results.extended_decision_values == 1, then a matrix .classifier_decision_values of dimension [num_resample_runs x num_cv_splits x num_test_points x num_train_times x num_test_times] will be returned that will have the decision values for the correct/actual class for each test point. If save_results.extended_decision_values = 2, then a matrix .all_classifier_decision_values is returned that has dimensions [num_resample_runs x num_cv_splits x num_test_points x num_classes x num_train_times x num_test_times] that contains all the decision values for every class (not just the correct class). Thus this result contains all the information from the whole decoding process (and consequentially it could take up a lot of disk space/memory). A matrix .all_classifier_decision_labels [num_resample_runs x num_cv_splits x num_test_points x num_train_times x num_test_times] will also be returned that contains all the labels that were used. From these two structures it is possible to derive all other decoding measures that are returned.\n\nROC_AUC_RESULTS\n\nThis structure contains results that measure the area under the receiver operator characteristic (ROC) curves that are created separately for each class from the decision values. ROC curves graph the proportion of positive test points correctly classified (true positive rate) as a function of the proportion of negative test points incorrectly classified (false positive rate). The area under this curve (ROC AUC) gives a measure of decoding accuracy that has a number of useful properties, including the fact that it is invariant to the ratio of positive to negative examples, and that it can be used to determine decoding accuracies when multiple correct classes are present at the same time. The results in this structure have the following fields:\n\n.separate_CV_ROC_results\n\n\nThis structure calculate the ROC AUC separately for each cross-validation split of the data. The advantage of calculating this separately for each cross-validation split is that it maintains the independence of the decoding results across cross-validation splits. The disadvantage is that most times there will only be one or a couple test points for each class which will lead to a highly variable estimate of the ROC AUC. For this reason it is often better to use the .combined_CV_ROC_results results described below.\n\n\n.combined_CV_ROC_results\n\n\nThis structure calculate the ROC AUC by combining all the decision values from all the test points across the different cross-validation splits. While the test points should all be independent from one another the classifiers used to evaluate these test points are highly related (since they are most likely trained using very similar data), thus these results could be slightly biased. However, since a much larger number of points are used to create these ROC curves, the results are more likely to be more sensitive (i.e., these results are slightly more likely to contain type 1 errors but much less likely to create type 2 errors compared to using the .separate_CV_ROC_result structure).\n\n\nBoth the .separate_CV_ROC_results .combined_CV_ROC_results have the following fields:\n\n\n.decoding_results\n\n\nThis contains a [num_resample_runs x (num_CV_splits) x num_classes x num_training_times x num_test_times] matrix that contains the ROC AUC results (for the separate_CV_ROC_results there are 5 ‘dimensions’ while for the combined_CV_ROC_results results there are only 4 dimensions since the results combine data from the different cross-validation splits when calculating the ROC AUC results).\n\n\n.mean_decoding_results\n\n\nThis is a [num_training_times x num_test_times] sized matrix that contains the mean ROC AUC values averaged over resample runs the different classes and for the separate_CV_ROC_results, the results are also averaged over CV splits.\n\n\n.stdev\n\n\nThis field contains the following measure of variability of the ROC AUC results:\n\n\n.over_classes(num_resample_runs, num_CV_splits, num_train_times, num_test_times)\n\n\nComputes the standard deviation over the results for each class.\n\n\n.over_resamples(num_train, num_test)\n\n\nTakes the mean over all the classes (and cross-validation runs), and computes the standard deviation over the resample runs. Additionally, .separate_CV_results has the following measure:\n\n\n.over_CVs(num_resample_runs, num_classes, num_train_times, num_test_times)\n\n\nTakes the mean over classes and computes the variability over the cross-validation runs.\n\n\nMUTUAL_INFORMATION\n\nThis structure contains results that measure the mutual information that is calculated from the confusion matrix (created from the 0-1 loss results). For more information on the relationship between mutual information and decoding see Quian Quiroga and Panzeri, Nature Reviews Neuroscience, 2009. The results in this structure have the following fields:\n\n.from_combined_confusion_matrix_over_all_resamples\n\n\nThe mutual information in this structure is calculated based on a confusion matrix that combines all the results from all the resample runs (i.e., it is the mutual information calculated from confusion matrix that is returned in the field ZERO_ONE_LOSS_RESULTS.confusion_matrix_results.confusion_matrix). This mutual information should be the most accuracy (and have the least bias during any baseline period), and the accuracy of these results should increase if more resample runs are used. The disadvantage of this estimate is that no estimate of the variability of this measure is possible since it uses data from all resample runs, thus one can not plot errorbars with this measure.\n\n\n.from_separate_confusion_matrix_for_each_resample\n\n\nThe mutual information in this structure is calculated based on a confusion matrix that is created separately for each resample run. This mutual information will most likely be biased upward due to limited sampling unless a very large number of test points are used (thus the mutual information in any baseline period is likely to be greater than 0). The advantage of using this method is that one can now estimate a measure of this mutual information’s variability calculated over resamples, which is in the field .stdev.over_resamples.",
    "crumbs": [
      "Toolbox Design",
      "Cross-validator",
      "standard_resample_cv"
    ]
  },
  {
    "objectID": "toolbox-design/data-formats/index.html",
    "href": "toolbox-design/data-formats/index.html",
    "title": "Data formats",
    "section": "",
    "text": "There are currently two data formats used by the Neural Decoding Toolbox, which are:\n\n\nRaster-format\n\n\n\n\nBinned-format",
    "crumbs": [
      "Toolbox Design",
      "Data formats"
    ]
  },
  {
    "objectID": "toolbox-design/data-formats/raster-format/index.html",
    "href": "toolbox-design/data-formats/raster-format/index.html",
    "title": "raster-format",
    "section": "",
    "text": "Raster-format data files contain information from a single site (e.g., data from one neuron, one LFP channel, etc.). Data that is in raster-format contains three variables: raster_data, raster_labels and raster_site_info, which are described below.\n\nraster_data\n\n\nThe variable raster_data is a [num_trials x num_times] matrix, where each row is data from one trial, and each column is data from one time point (relative to some event that the data was aligned to). The trials should be in the order that they were shown to a subject, i.e., the first row of the matrix should come from the first trial, the second row from the second trial, etc.. For spiking data, this matrix will contain a 1 at every time there is a spike and a zero at every time there was not a spike. For continuous data, such as LFPs, MEG data, etc., each point in a row should be the continuous LFP/MEG value that is recorded on each trial.\n\n\nraster_labels\n\n\nThe variable raster_labels is a structure that contains cell arrays of strings (or vectors of numbers) that indicate the labels for what conditions were present on each trial, i.e., raster_labels.variable_name1, raster_labels.variable_name2, etc., would all be cell arrays (or vectors) of length [num_trials x 1], which each entry indicates what experimental condition occurred on a given trial. Having a structure with cell arrays that contain different labels is useful because it allows one to decoding different information from the data. For example, suppose we had an experiment in which images of ten different people and images of ten different cars were shown. Then we could have the variable raster_labels.stimulus_ID that had 20 unique string names corresponding to each exact person or car image that was shown on each trial. Additionally, we could have a second raster_labels.face_or_car that contained only the strings ‘face’, or ‘car’ that indicated whether a face or a car was shown on each trial. It would then be possible to run two decoding analyses, one in which we try to decode each image regardless of whether it is a face or a car (where chance would be 1/20), and a second analysis in which we try to decode whether an image is a face or a car (where chance would be 1/2).\n\n\nraster_site_info\n\n\nThe variable raster_site_info is a structure that contains any additional information about the site that should be saved. For example, this structure could contain information about the date that data was recorded on (e.g., raster_site_info.date_recorded = 2011_9_28), or number to identify the session that the data was recorded from (e.g., raster_site_info.sessionID = 200), or information about the spike sorting quality (e.g., raster_site_info.isMUA = 1), etc.. At the moment any variable is allowed in this structure (and it is not strictly necessary to have this structure for the decoding algorithms to run).",
    "crumbs": [
      "Toolbox Design",
      "Data formats",
      "raster-format"
    ]
  },
  {
    "objectID": "toolbox-design/feature-preprocessors/zscore_normalize_fp/index.html",
    "href": "toolbox-design/feature-preprocessors/zscore_normalize_fp/index.html",
    "title": "zscore_normalize_fp",
    "section": "",
    "text": "This feature preprocessor object applies z-score normalization to each feature by calculating the mean and the standard deviation for each feature using the training data, and then subtracting the mean and dividing by the standard deviation for each feature in the training and test sets. This function is useful for preventing some classifiers from relying too heavily on particular features when different features can have very different ranges of values (for example, it is useful when decoding neural data because different neurons can have different ranges of firing rates).\n\nMethods\n\n\nfp = zscore_normalize_FP\n\n\nBasic constructor\n\nThe following methods are used by the cross-validator algorithm to apply feature preprocessing to the data:\n\n[fp XTr_normalized] = set_properties_with_training_data(fp, XTr, ~)\n\n\nCalculates the normalization parameters on the training data (i.e., the mean and the standard deviation) and returns the z-score normalized training data in XTr_normalized (by subtracting the mean and dividing by the standard deviation).\n\n\nX_normalized = preprocess_test_data(fp, X_data)\n\n\nNormalized the test data (X_data) using the mean and standard deviation learned on the training data.\n\n\ncurrent_FP_info_to_save = get_current_info_to_save(fp)\n\n\nReturns an empty matrix indicating that there is no additional information to save for this feature preprocessor.",
    "crumbs": [
      "Toolbox Design",
      "Feature-preprocessors",
      "zscore_normalize_fp"
    ]
  },
  {
    "objectID": "toolbox-design/feature-preprocessors/select_pvalue_significant_features_fp/index.html",
    "href": "toolbox-design/feature-preprocessors/select_pvalue_significant_features_fp/index.html",
    "title": "select_pvalue_significant_features_fp",
    "section": "",
    "text": "This feature preprocessor object applies an ANOVA separately to each feature using data from the training set to find the p-value of all features. It then removes all features that have p-values greater than a specified threshold from the training and test sets.\n\nMethods\n\n\nfp = select_pvalue_significant_features_FP\n\n\nBasic constructor\n\n\nfp = set_pvalue_threshold(fp, X_data)\n\n\nThis method sets the threshold so that all features with ANOVA p-values less than this threshold will be used, and all features with p-values greater than this threshold will be excluded. This method must be called prior to using the object to normalize data.\n\n\nfp = save_extra_preprocessing_info(save_extra_info)\n\n\nIf this method is passed a value of 1, the p-values for all features will be saved and returned along with the classifier results. It should be noted that setting this value to 1 will greatly increase the size of the results file.\n\nThe following methods are used by the cross-validator algorithm to apply feature preprocessing to the data:\n\n[fp XTr_preprocesed] = set_properties_with_training_data(fp, XTr, YTr)\n\n\nCalculates the p-values for each feature by applying an ANOVA to the training data, and returns potentially lower dimensional training data in XTr_normalized due to less selective feature being removed.\n\n\nX_preprocessed = preprocess_test_data(fp, X_data)\n\n\nRemoves the features that were considered non-selective from the test data (X_data) (where the non-selective features were determined previously by running an ANOVA on the training data), and returns the modified data in X_preprocessed.\n\n\ncurrent_FP_info_to_save = get_current_info_to_save(fp)\n\n\nReturns the p-values from ANOVAs applied to each feature in the structure current_preprocessing_information_to_save.the_p_values_org_order. These values will then be saved by the cross-validator.",
    "crumbs": [
      "Toolbox Design",
      "Feature-preprocessors",
      "select_pvalue_significant_features_fp"
    ]
  },
  {
    "objectID": "toolbox-design/classifiers/max_correlation_coefficient_cl/index.html",
    "href": "toolbox-design/classifiers/max_correlation_coefficient_cl/index.html",
    "title": "max_correlation_coefficient_cl",
    "section": "",
    "text": "This CL object learns a mean population vector (template) for each class from the training set (by averaging together the all training points within each class). The classifier is tested by calculated Pearson’s correlation coefficient between a test point and the templates learned from the training set, and the class with the highest correlation value is returned as the predicted label. The decision values returned by the classifier are the correlation coefficients between all test points and all templates.\n\nNote: if the data has only one feature then the class with the smallest squared difference between the learned (one-dimensional) templates and the test point is chosen (and the decision values are the negative of these squared differences).\n\n\nMethods\n\n\ncl = max_correlation_coefficient_CL\n\n\nBasic constructor\n\n\ncl = train(cl, XTr, YTr)\n\n\nReturns the trained classifier (which has learned the mean template vector for each class).\n\n\n[predicted_labels decision_values] = test(cl, XTe)\n\n\nPredicts the class of each test point (XTe) by correlated each test point with the learned templates and returns the class with the highest correlation (if two classes are tied for the highest correlation then one of the two classes is randomly chosen).",
    "crumbs": [
      "Toolbox Design",
      "Classifiers",
      "max_correlation_coefficient_cl"
    ]
  },
  {
    "objectID": "toolbox-design/classifiers/index.html",
    "href": "toolbox-design/classifiers/index.html",
    "title": "Classifiers",
    "section": "",
    "text": "Classifiers (CL) take a set of training data and training labels, and learn a model of the relationship between the training data and the labels from the different classes. Once this model has been learned (i.e., once the classifier has been trained), the classifier is then used to make predictions about what labels were present in a new set of ‘test data’.\nThe Neural Decoding Toolbox comes with following classifier objects:",
    "crumbs": [
      "Toolbox Design",
      "Classifiers"
    ]
  },
  {
    "objectID": "toolbox-design/classifiers/index.html#methods-that-must-be-implemented",
    "href": "toolbox-design/classifiers/index.html#methods-that-must-be-implemented",
    "title": "Classifiers",
    "section": "Methods that must be implemented",
    "text": "Methods that must be implemented\nObjects that are classifiers must implement the following two methods:\n\ncl = cl.train(XTr, YTr)\n\n\nThis method takes the training data (XTr) and the training labels(YTr) and learns a model of the relationship between XTr and YTr. XTr is a num_features x num_training_points sized matrix, and YTr is a num_training_points x 1 dimensional vector. Different classifiers model the relationship between XTr and YTr in different ways, and thus yield different predictions on the test set.\n\n\n[predicted_labels decision_values] = cl.test(XTe)\n\n\nThis method takes in test data (XTe), and uses the model learned from the training set to make predictions about which label corresponds to each test data point (where XTe is a num_features x num_test_points  sized matrix).The predictions are returned in the variable ‘predicted_labels’. The classifier also returns a num_test_point x num_classes matrix of ‘decision_values’ that contains values that indicate how confident the classifier is that a given test point came from each classes. The decision_values matrix is useful for computing other measure of classification performance (apart from the 0-1 loss measure is most commonly used). This method can return an empty matrix of decision values in which case any additional classification measure will not be computed.",
    "crumbs": [
      "Toolbox Design",
      "Classifiers"
    ]
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "The Neural Decoding Toolbox was developed by Ethan Meyers in Center for Brains Minds and Machines  at MIT.\n\nBelow are some relevant links:\n\n\nNews about the latest features of the toolbox.\n\n\nA list of publications that use the toolbox\n\n\nA sitemap for this website"
  },
  {
    "objectID": "tutorials/introduction-tutorial/index.html",
    "href": "tutorials/introduction-tutorial/index.html",
    "title": "Introduction tutorial",
    "section": "",
    "text": "The following tutorial gives a basic introduction to the data formats used by Neural Decoding Toolbox (NDT) and shows how to run a simple decoding analysis. The tutorial is based on a dataset collected by Ying Zhang in Bob Desimone’s lab at MIT which is downloaded here.",
    "crumbs": [
      "Tutorials",
      "Introduction tutorial"
    ]
  },
  {
    "objectID": "tutorials/introduction-tutorial/index.html#overview-of-the-ndt",
    "href": "tutorials/introduction-tutorial/index.html#overview-of-the-ndt",
    "title": "Introduction tutorial",
    "section": "Overview of the NDT",
    "text": "Overview of the NDT\n\nNeural decoding is a process in which a pattern classifier learns the relationship between neural activity and experimental conditions using a training set of data. The reliability of the relationship between the neural activity and experimental conditions is evaluated by having the classifier predict what experimental conditions were present on a second test set of data.\n\n\nThe NDT is built around 4 different object classes that allow users to apply neural decoding in a flexible and robust way. The four types of objects are:\n\n\n\nDatasources (DS) which generate training and test splits of the data.\n\n\nFeature preprocessors (FP) which apply preprocessing to the training and test splits.\n\n\nClassifiers (CL) which learn the relationship between experimental conditions and data on the training set, and then predict experimental conditions on the test data.\n\n\nCross-validators (CV) which take the DS, FP and CL objects and run a cross-validation decoding procedure.\n\n\n\nThe NDT comes with a few implementations of each of these objects, and defines interfaces that allow one to create new objects that extend the basic functionality of the four object classes. More information about the design of the NDT and these four objects classes can be found here.\n\n\nThe following tutorial explains the data formats used by the Neural Decoding Toolbox, and how to run a decoding experiment using the basic versions of the four object classes.\n\n\nAbout the data\n\n\nThe data used in this tutorial was collected by Ying Zhang in Bob Desimone’s lab at MIT and was used in the supplemental figures in the paper Object decoding with attention in inferior temporal cortex, PNAS, 2011. The data consists of single unit recordings from the 132 neurons in inferior temporal cortex (IT). The recordings were made while a monkey viewed 7 different objects that were presented at three different locations (the monkey was also shown images that consisted of three objects shown simultaneously and had to perform an attention task, however for the purposes of this tutorial we are only going to analyze data from trials when single objects were shown). Each object was presented approximately 20 times at each of the three locations. The data can be downloaded here.\n\n\n\n\nAdding the toolbox path\n\nBefore using any of the functions in the NDT, the path must be set so that Matlab knows where to find these functions. The function add_ndt_paths_and_init_rand_generator adds the path and the appropriate directories that contain the different NDT functions. Additionally, this function initializes the random number generator (to the current time on the CPU’s clock) so that each time the toolbox is used a different sequence of random numbers will be generated (by default Matlab uses the same seed to initialize the random number generator, which leads to the same sequence of random numbers every time Matlab is started). The following lines show how to use add_ndt_paths_and_init_rand_generator:\n\n\n% add the path to the NDT so add_ndt_paths_and_init_rand_generator can be called\ntoolbox_basedir_name = 'ndt.1.0.4/'\naddpath(toolbox_basedir_name);\n\n% add the NDT paths using add_ndt_paths_and_init_rand_generator\nadd_ndt_paths_and_init_rand_generator",
    "crumbs": [
      "Tutorials",
      "Introduction tutorial"
    ]
  },
  {
    "objectID": "tutorials/introduction-tutorial/index.html#data-formats",
    "href": "tutorials/introduction-tutorial/index.html#data-formats",
    "title": "Introduction tutorial",
    "section": "Data formats",
    "text": "Data formats\n\nIn order to use the NDT, the neural data must be in a usable format. Typically this involves putting the data in raster-format and then converting it to binned-format using the create_binned_data_from_raster_data function that is found in the tools directory. Information about these data formats is described here.\n\n\n\nRaster format\n\nTo run a decoding analysis using the NDT you first need to have your data in a usable format. In this tutorial we will use data collected by Ying Zhang in Bob Desimone’s lab at MIT. The directory Zhang_Desimone_7objects_raster_data/ contains data in raster-format. Each file in this directory contains data from one neuron. To start, let us load one of these files and examine its contents by typing the command:\n\n\nload bp1021spk_04B_raster_data.mat\n\n\nData that is in raster-format contains three variables: raster_site_info, raster_labels, and raster_data. The variable raster_data is a matrix where each row corresponds to the data from one trial, and each column corresponds to data from one time point (the rows are also in order so that the first trial is in the first row, and the last trial is in the last row). Because we are dealing with neural spiking data in this tutorial each column in the matrix that was just loaded corresponds to a time when a spike occurred. We can view the spike rasters from each trial and a peri-stimulus time histogram (PSTH) of the data by typing the following commands:\n\n\n% view the rasters from one neuron\nsubplot(1, 2, 1)\nimagesc(~raster_data); colormap gray\nline([500 500], get(gca, 'YLim'), 'color', [1 0 0]);\nylabel('Trials')\nxlabel('Time (ms)')\ntitle('rasters')\n\n% view the PSTH for one neuron\nsubplot(1, 2, 2)\nbar(sum(raster_data));\nline([500 500], get(gca, 'YLim'), 'color', [1 0 0]);\nylabel('Number of spikes')\nxlabel('Time (ms)')\ntitle('PSTH')\n\n\nFrom looking at the PSTH, one can see that this cell increased its firing rate shortly after stimulus onset (the stimulus onset was at 500 ms).\n\n\nThe structure raster_labels is a structure that contains cell arrays that lists the experimental conditions that were present on each trial (each cell array has as many entries as there are rows in the raster_data matrix, so that there is an experimental condition label for each trial). For example, the variable raster_labels.stimulus_ID contains the labels for which of the 7 stimuli was shown on each trial, and the variable raster_labels.stimulus_position contains the position where the stimulus was shown.\n\n\nThe structure rater_site_info contains any additional information about the recording site that the experimenter wants to record. For example, one could keep a record of the quality of the spike sorting isolation in this structure, or information about the position of where the neuron was recorded relative to a grid system used, etc.. For the purposes of this tutorial we will ignore this structure.\n\n\n\n\nBinning the data\n\nThe NDT decoding objects operate on data that is in binned-format. To convert data in raster-format to binned-format, we can use the tool create_binned_data_from_raster_data, which calculates the average firing rate of neurons over specified intervals and sampled with a specified frequency (i.e., a boxcar filter is used). create_binned_data_from_raster_data takes in four arguments: 1) the name of the directory where the raster-format data is stored, 2) the name (potentially including a directory) that the binned data should be saved as, 3) a bin size that specifies how much time the firing rates should be calculated over, and 4) a sampling interval that specifies how frequently to calculate these firing rates. To calculate the average firing rates in 150 ms bins sampled every 50 ms, the following commands can be used:\n\n\nraster_file_directory_name = 'Zhang_Desimone_7objects_raster_data/'\nsave_prefix_name = 'Binned_Zhang_Desimone_7object_data';\nbin_width = 150;\nstep_size = 50;\n\ncreate_binned_data_from_raster_data(raster_file_directory_name, save_prefix_name, bin_width, step_size);\n\n\nThe output of this function will be a file called Binned_Zhang_Desimone_7object_data_150ms_bins_50ms_sampled.mat. Data in binned-format has similar fields to data in raster-format except that data from all the neurons are now grouped together into single structures. The three variables for binned-format data are: 1) the_data{} which is a cell array where each entry contains a [num_trials x num_bins] matrix of data, which is a binned version of the raster_data for each neuron; 2) binned_labels which is a structure that contains cell array for the labels for each neuron, and 3) binned_site_info which contains all the extra info for each neuron.\n\n\n\n\nDetermining number of condition repetitions\n\nBefore beginning the decoding analysis it is useful to know how many times each experimental condition (e.g., stimulus) was presented to each site (e.g., neuron). In particular, it is useful to know how many times the condition that has the fewest repetitions was presented. To do this we will use the tool find_sites_with_k_label_repetitions which finds all sites that have at least k repetitions using data that is in binned-format. Below we count the number of sites with k repetitions for different numbers of k, and store them in the variable num_sites_with_k_repeats.\n\n\n% load the binned data\nload Binned_Zhang_Desimone_7object_data_150ms_bins_50ms_sampled.mat\n\nfor k = 1:65\n&nbsp; &nbsp; inds_of_sites_with_at_least_k_repeats = find_sites_with_k_label_repetitions(binned_labels.stimulus_ID, k);\n&nbsp; &nbsp; num_sites_with_k_repeats(k) = length(inds_of_sites_with_at_least_k_repeats);\nend\n\n\nBased on these results we see that all of the 132 sites have 59 repetitions of all 7 of the stimuli, and that 125 sites have 60 repetitions of all 7 stimuli. This information is useful when deciding how many cross-validations splits to use, as described below.",
    "crumbs": [
      "Tutorials",
      "Introduction tutorial"
    ]
  },
  {
    "objectID": "tutorials/introduction-tutorial/index.html#running-the-analysis",
    "href": "tutorials/introduction-tutorial/index.html#running-the-analysis",
    "title": "Introduction tutorial",
    "section": "Running the analysis",
    "text": "Running the analysis\n\nPerforming a decoding analyses involves several steps:\n\n\n\ncreating a datasource (DS) object that generates training and test splits of the data.\n\n\noptionally creating feature-preprocessor (FP) objects that learn parameters from the training data, and preprocess the training and test data.\n\n\ncreating a classifier (CL) object that learns the relationship between the training data and training labels, and then evaluates the strength of this relationship on the test data.\n\n\nrunning a cross-validator object that using the datasource (DS), the feature-preprocessor (FP) and the classifier (CL) objects to do a cross-validation procedure that estimates the decoding accuracy.\n\n\n\nBelow we describe how to create and run these objects on the Zhang-Desimone dataset.\n\n\n\nDatasources (DS)\n\nA datasource object is used by the cross-validator to generate training and test splits of the data. Below we create a basic_DS object that takes binned-format data, a cell array of labels, and a scalar that specifies how many cross-validation splits to use. The default behavior of this datasource is to create test splits that have one example of each object in them and num_cv_splits - 1 examples of each object in the training set.\n\n\nAs calculated above, all 132 neurons have 59 repetitions of each stimulus, and 125 neurons have 60 repetitions of each stimulus. Thus we can use up to 59 cross-validation splits using all neurons, or we could set the datasource to use only a subset of neurons and use 60 cross-validation splits. For the purpose of this tutorial, we will use all the neurons and only 20 cross-validation splits (to make the code run a little faster). The basic_DS datasource object also has many more properties that can be set, including specifying that only certain labels or neurons should be used. More information about this object can be found here.\n\n\n% the name of the file that has the data in binned-format\nbinned_format_file_name = 'Binned_Zhang_Desimone_7object_data_150ms_bins_50ms_sampled.mat'\n\n% will decode the identity of which object was shown (regardless of its position)\nspecific_label_name_to_use = 'stimulus_ID';\n\nnum_cv_splits = 20;\n\nds = basic_DS(binned_format_file_name, specific_label_name_to_use, num_cv_splits)\n\n\n\n\nFeature-preprocessors (FP)\n\nFeature preprocessors use the training set to learn particular parameters about the data, and then applying some preprocessing to the training and test sets using these parameters. Below will we create a zscore_normalize_FP that zscore normalizes the data so that each neuron’s activity has approximately zero mean and a standard deviation of 1 over all trials. This feature-preprocessor is useful so that neurons with high firing rates do not end up contributing more to the decoding results than neurons with lower firing rates when a max_correlation_coefficient_CL is used.\n\n\n% create a feature preprocessor that z-score normalizes each neuron\n\n% note that the FP objects are stored in a cell array \n% which allows multiple FP objects to be used in one analysis\n\nthe_feature_preprocessors{1} = zscore_normalize_FP;\n\n\n\n\nClassifiers (CL)\n\nClassifiers take a “training set” of data and learn the relationship between the neural responses and the experimental conditions (labels) that were present on particular trials. The classifier is then used to make predictions about what experimental conditions are present on trials from a different “test set” of neural data. Below we create a max_correlation_coefficient_CL classifier which learns prototypes of each class k that consists of the mean of all training data from class k. The predicted class for a new test point x is the class that has the maximum correlation coefficient value (i.e., the smallest angle) between the x and each class prototype.\n\n\n% create the CL object\nthe_classifier = max_correlation_coefficient_CL;\n\n\n\n\nCross-validators (CV)\n\nCross-validator objects take a datasource, a classifier and optionally feature-preprocessor objects and run a decoding procedure by generating training and test data from the datasource, preprocessing this data with the feature-preprocessors and then training and testing the classifier on the resulting data. This procedure is run in two nested loops. The inner ‘cross-validation’ loop runs a cross-validation procedure where the classifier is trained and tested on different divisions of the data. The outer, ‘resample’ loop generates new splits (and also potentially pseudo-populations) of data, which are then run in a cross-validation procedure by the inner loop. Below we create a standard_resample_CV object that runs this decoding procedure.\n\n\n% create the CV object\nthe_cross_validator = standard_resample_CV(ds, the_classifier, the_feature_preprocessors);\n\n% set how many times the outer 'resample' loop is run\n% generally we use more than 2 resample runs which will give more accurate results\n% but to save time in this tutorial we are using a small number.\n\nthe_cross_validator.num_resample_runs = 2;\n\n\n\n\nRunning the decoding\n\nTo run the decoding procedure we call the cross-validator’s run_cv_decoding method, and the results are saved to a structure DECODING_RESULTS.\n\n\n% run the decoding analysis\nDECODING_RESULTS = the_cross_validator.run_cv_decoding;\n\nsave_file_name = 'Zhang_Desimone_basic_7object_results'\n\nsave(save_file_name, 'DECODING_RESULTS');",
    "crumbs": [
      "Tutorials",
      "Introduction tutorial"
    ]
  },
  {
    "objectID": "tutorials/introduction-tutorial/index.html#plotting-the-results",
    "href": "tutorials/introduction-tutorial/index.html#plotting-the-results",
    "title": "Introduction tutorial",
    "section": "Plotting the results",
    "text": "Plotting the results\n\nBelow we show how to plot the decoding accuracies as function of time using the plot_standard_results_object which is useful when comparing decoding accuracies from different analyses. We also show how to plot the results when training the classifier at one time and testing the classifier at a second time (ie., a temporal-cross-training plot) using the plot_standard_results_TCT_object object, which is useful for testing where information is contained in a dynamic population code.\n\n\n\nPlot decoding accuracy\n\nTo plot basic decoding results as a function of time, we will use the plot_standard_results_object. This object takes the decoding result files that were created by the standard_resample_CV object and plots them in a nice way. There are many properties that can be set for this object, so we recommend you read the documentation to see all the possibilities. Below we show how to plot the results we created above setting only a few of the possible parameters.\n\n\nresult_names{1} = save_file_name; &nbsp;\n\n% create the plot results object\nplot_obj = plot_standard_results_object(result_names);\n\n% put a line at the time when the stimulus was shown\nplot_obj.significant_event_times = 0;\n\n% display the results\nplot_obj.plot_results;\n\n\nOther measures of decoding accuracy can be plotted by setting the property plot_obj.result_type_to_plot. For example, if this property is set to 6, then mutual information will be plotted, and if this property is to 2 normalized rank results will be plotted.\n\n\n\n\nTemporal-cross-decoding\n\nTo plot a matrix of decoding accuracies showing the results when the classifier was trained at time t1 and tested at time t2 we will use the plot_standard_results_TCT_object. The basic functions of this object are similar to the plot_standard_results_object, namely it takes the name of a decoding result file that was generated by running standard_resample_CV object and plots the full temporal-cross-training (TCT) matrix. There are also many properties that can be set for this object, so we again recommend you read the documentation to see all the possibilities. Below we show again how to plot the results we created above setting only a few of the possible parameters.\n\n\n% create the plot results object\n% note that this object takes a string in its constructor not a cell array\nplot_obj = plot_standard_results_TCT_object(save_file_name);\n\n% put a line at the time when the stimulus was shown\nplot_obj.significant_event_times = 0;\n\n% display the results\nplot_obj.plot_results;",
    "crumbs": [
      "Tutorials",
      "Introduction tutorial"
    ]
  },
  {
    "objectID": "tutorials/introduction-tutorial/index.html#conclusion",
    "href": "tutorials/introduction-tutorial/index.html#conclusion",
    "title": "Introduction tutorial",
    "section": "Conclusion",
    "text": "Conclusion\n\nThis concludes the introductory tutorial. You should now understand the design of the Neural Decoding Toolbox and how to do a basic decoding analysis. We recommend trying out this tutorial yourself in Matlab and experimenting with different datasource, feature-preprocessor, cross-validator and plotting parameters. Once you feel comfortable with this tutorial you can look at the generalization analysis tutorial which shows how to test whether neural representations contain information in an abstract/invariant format, or you can look at the getting started with your own data tutorial which shows the steps necessary to being analyzing your own data.",
    "crumbs": [
      "Tutorials",
      "Introduction tutorial"
    ]
  },
  {
    "objectID": "tutorials/getting-started-with-your-own-data/index.html",
    "href": "tutorials/getting-started-with-your-own-data/index.html",
    "title": "Getting started with your data",
    "section": "",
    "text": "This tutorial shows how to begin to run decoding experiments using your own data. Most of the steps below are similar to those described in the basic tutorial so we recommend going through that tutorial first since it describes the several of the steps below in more detail.\n\nFormatting your data\nIn order to use the Neural Decoding Toolbox, your data must be in the proper format. The easiest way to start is to put your data in raster-format. Data that is in raster-format contains separate files for each site (by ‘site’ we mean recorded data, such as single unit activity of a neuron, multi-unit activity of a site, LFP power from one recorded channel, MEG activity from one recorded channel, one voxel from an fMRI analysis, etc.). Each file (from one site) that is in raster-format contains the three variables that should be named: raster_data, raster_labels, andraster_site_info that are described below (for more information on raster-format please see the data-formats and raster-format pages).\nThe variable raster_data is a matrix where each row contains data from one trial, and each column contains data from one time point (i.e.,raster_data is a [num_trials x num_time_points] matrix). The data from each trial (row) should be aligned to a particular experimental event. For example, in the Zhang-Desimone dataset, the raster-format data is aligned to a time when a stimulus was shown, and each column corresponds to 1 millisecond of data, with a 1 indicating a spike occurred and a 0 indicating a spike did not occur. We highly recommend including activity from a baseline period before the time when the decoding variable is present; when you run your decoding analysis, the decoding accuracy during the baseline period should be at chance, which is a good sanity check that everything is working properly. For example, in the Zhang-Desimone dataset, the stimulus was shown 500 time points into the data giving us a clear baseline period where we should get chance decoding performance.\nThe variable raster_labels is a structure that contains the different experimental conditions that were present on each trial. Each field raster_labels.experiment_variable_k is a num_trials length cell array of strings (or vector of numbers) that contains the experimental condition that occurred on each trial. For example, in the Zhang-Desimone data, the position and identity of each stimulus that were shown on each trial are contained in the variables raster_labels.stimulus_position and raster_labels.stimulus_ID.\nThe final variable, raster_info contains any additional information about the site that should be recorded. While technically this variable can be set to an empty matrix and the Neural Decoding Toolbox will still run, we usually find it highly valuable to record much additional information about the experiment here so to have a complete record of the experiment. Typically this variable will contain information such as the date that the recording was made, the quality of the recording (e.g., if it is single unit, or multi-unit activity), the brain region where the recording is made, etc.. This information can be useful for later analyses, for example, if one wants to run an analysis using only data from a particular brain region, or only using single unit sites, etc..\nAll files in raster-format from all recorded sites should be put together in a directory (called something like my_data_raster_format/).\n\n\nBinning your data\nOnce you have all your data in raster-format stored in a particular directory, you can convert it to binned-format that is used by the Neural Decoding Toolbox using the helper function create_binned_data_from_raster_data function. Data that is in binned-format is similar to data in raster format (e.g., there are three variables named binned_data, binned_labels, and binned_site_info that are similar to the raster-format variables) except in binned-format data from all sites are contained together in a cell array (for more information see the page on binned-format.\nTo convert data from raster-format to binned-format the following code can be used.\n\ncreate_binned_data_from_raster_data('my_data_raster_format/', 'My_Binned_Data', 150, 50);\n\nIn the above example, my_data_raster_format/ is the directory that contains the raster files, My_Binned_Data is a name that will be added to the saved binned-format data, 150 refers to the bin size that the data will be averaged over, and 50 refers to the sampling interval for creating averaged bins. The output from running this function will be a file called My_Binned_Data_150ms_bins_50ms_sampled  that contains the data in binned-format. For more information about the binning function see the introduction tutorial section on binning the data and the create_time-averaged_binned_data_from_raster_data function documentation.\n\n\nCalculating how many cross-validation splits to use\nIn a cross-validation decoding analyses, data is split into k different sections, and a classifier is trained on k-1 of these sections and needs to make predictions on the remaining section. The default setting for the basic_DS and generalization_DS, is to have each of the k sections contain 1 example of data from each experimental condition that will be decoding. Thus, for example, if one wants to have k=5 splits of the data, then only sites that have 5 repetitions of each experimental condition can be used in the analysis. To determine how many sites have at least k repetitions of each experimental condition, the function find_sites_with_k_label_repetitions can be used as follows:\n\n% load the binned data\nload My_Binned_Data_150ms_bins_50ms_sampled.mat\n\nfor k = 1:65\n    inds_of_sites_with_at_least_k_repeats = find_sites_with_k_label_repetitions(binned_labels.stimulus_ID, k);\n    num_sites_with_k_repeats(k) = length(inds_of_sites_with_at_least_k_repeats);\nend\n\nBy looping over different values of k and storing the results in num_sites-With_k_repeats one can see how many sites have 1, 2, … etc, repetitions. Decoding accuracy increases as more sites are used, and as more splits of the data are used (i.e., using a larger k). Thus one should try to find a k that is as large as possible but that still allows one to use most of the data you have. Determining the final k is a little bit of an art, although the decoding analyses should be fairly robust to a range of k values, provided k is not too small (e.g., at least 5), and that there are still a significant number of sites used (e.g., for neural data, at least 100 sites).\n\n\nRunning a decoding analysis\nThe final stage involves running a decoding analyses. The type of analysis can be a simple decoding of a particular experimental variable as described in the introduction tutorial, or it can be a more complex analysis such as examining how invariant neural activity is to particular experimental conditions, as described the generalization analysis tutorial. Below we give some code that describes how to run a simple analysis for decoding a particular experimental condition. For more details on this code, see the introduction tutorial.\n\n% add the path to include the code for using the NDT\ntoolbox_basedir_name = 'ndt.1.0.0/'\naddpath(toolbox_basedir_name);\nadd_ndt_paths_and_init_rand_generator\n\n% the name of your binned-format data\nbinned_data_file_name = 'My_Binned_Data_150ms_bins_50ms_sampled.mat'\n\n% select labels to decode \nspecific_label_name = 'my_variable_to_decode';\n\n% choose the number of cross-validation section as determined above\nnum_cv_splits = 20;  \n\n% create a basic datasource\nds = basic_DS(binned_data_file_name, specific_label_name, num_cv_splits)\n\n% create a feature proprocessor and a classifier\nthe_feature_preprocessors{1} = zscore_normalize_FP;\nthe_classifier = max_correlation_coefficient_CL;\n\n% create a cross-validation object\nthe_cross_validator = standard_resample_CV(ds, the_classifier, the_feature_preprocessors);\n\n% run the decoding analysis\nDECODING_RESULTS = the_cross_validator.run_cv_decoding;\n\n% save the datasource parameters for our records\nDATASOURCE_PARAMS = ds.get_DS_properties;\n\n% save the decoding results as 'My_Decoding_Results\nsave('My_Decoding_Results', 'DECODING_RESULTS', 'DATASOURCE_PARAMS');\n\n% plot the results\nplot_obj = plot_standard_results_object({'My_Decoding_Results.mat'});\nplot_obj.plot_results;",
    "crumbs": [
      "Tutorials",
      "Getting started with your data"
    ]
  },
  {
    "objectID": "tutorials/generalization-analysis/index.html",
    "href": "tutorials/generalization-analysis/index.html",
    "title": "Generalization analysis",
    "section": "",
    "text": "The following tutorial shows how to use the Neural Decoding Toolbox (NDT) to conduct a generalization analysis which can test whether a population of neural activity contains information in a invariant/abstract representation. In particular, we will use the Zhang-Desimone 7 object dataset to examine how invariant neural representations in the Inferior Temporal cortex are to changes in the position of stimuli. This tutorial assumes that one is already familiar with the basics of the NDT as covered in the introductory tutorial.",
    "crumbs": [
      "Tutorials",
      "Generalization analysis"
    ]
  },
  {
    "objectID": "tutorials/generalization-analysis/index.html#invariant-representations",
    "href": "tutorials/generalization-analysis/index.html#invariant-representations",
    "title": "Generalization analysis",
    "section": "Invariant representations",
    "text": "Invariant representations\nAn important step in solving many complex tasks involves creating abstract/invariant representations from widely varying input patterns. For example, in order to act appropriately in social settings it is important to be able to recognize individual people. However the images of a particular person that is projected on our retinas can be very different due to the fact that the person might be at different distances from us, in different lighting conditions, etc. Thus at some level in our brains, there must be a neural representation that has abstracted away all the details present in particular images, in order to create abstract/invariant representations that are useful for behavior.\nA powerful feature of the Neural Decoding Toolbox is that it can be used to test whether a particular population of neural activity has created representations that are invariant to particular transformations. To do such a ‘generalization analysis’, one can train a classifier under one set of conditions, and then see if the classifier can generalize to a new related set of conditions in which a particular transformation has been applied. The datasource object generalization_DS is designed for this purpose and we will explain how to use it below.\n\nPosition invariance\nIn this tutorial we will use the Zhang-Desimone 7 object dataset to test how invariant neural representations in Inferior Temporal (IT) cortex are to changes in the retinal position of objects. The Zhang-Desimone 7 object dataset consists of neural responses to 7 different objects that were shown to a monkey at three different retinal locations. To test how invariant neural representations in IT are, we will train the classifier with data from one location, and then test the classifier either at the same location or at a different location. If the neural representation in IT are invariant to position, then training and testing a classifier at different locations should yield an equal level of performance as training and testing a classifier at the same location.\n\n\nBinning the data\nBefore starting this tutorial, make sure that the path to the NDT has been set as described here. For this tutorial we will use binned-format data that consists of the firing rate in a 400 ms window that starts 100 ms after the onset of the stimulus. The following code shows how to create this binned data.\n\n% change the line below to the directory where your raster format data .mat files are stored\nraster_file_directory_name = 'Zhang_Desimone_7objects_raster_data/' \nsave_prefix_name = 'Binned_Zhang_Desimone_7objects_data';\n\nbin_width = 400;\nstep_size = 400;\nstart_time = 601;\nend_time = 1000;\n\ncreate_binned_data_from_raster_data(raster_file_directory_name, save_prefix_name, bin_width, step_size, start_time, end_time);\n\n\n\n\nClassifiers/preprocessors\nNext we will create a classifier object and a preprocessor object. We will use the same classifier and preprocessor as was used in the basic tutorial.\n\nthe_classifier = max_correlation_coefficient_CL;\nthe_feature_preprocessors{1} = zscore_normalize_FP;\n\n\n\nThe generalization_DS\nIn order to train with data from one location and test with data from a different location we will use the generalization_DS datasource. To use this datasource, we first need to specify which labels belong which each training class and which labels belong to each test class using two cell arrays, which we will call the_training_label_names, and the_test_label_names respectively. Each entry in the cell array corresponds to the labels for one class. For example, if we set the_training_label_names{1} = {‘car_upper’}, and the_test_label_names{1} = {‘car_lower’}, this means that the first class will be trained with data from trials when the car was shown in the upper position, and the first class will have test data from trials in which cars where shown in the lower position.\nTo create training data for each object identity at location 1, and test data for each object at location 3, we can use the following code:\n\nid_string_names = {'car', 'couch', 'face', 'kiwi', 'flower', 'guitar', 'hand'};\n\nfor iID = 1:7   \n   the_training_label_names{iID} = {[id_string_names{iID} '_upper']};\n   the_test_label_names{iID} = {[id_string_names{iID} '_lower']};\nend\n\nNow that we have created cell array that specify which labels are to be mapped on to which classes, we can create the generalization_DS object. The first three arguments to the constructor of this object are the same as those used for the basic_DS object, and the last two arguments are the training and test label remapping cell arrays we just created.\n\n \nnum_cv_splits = 18;\n\nbinned_data_file_name = 'Binned_Zhang_Desimone_7objects_data_400ms_bins_400ms_sampled_601start_time_1000end_time';\nspecific_labels_names_to_use = 'combined_ID_position';  % use the combined ID and position labels\n\nds = generalization_DS(binned_data_file_name, specific_labels_names_to_use, num_cv_splits, the_training_label_names, the_test_label_names);\n\n\nWe can then create a cross-validator as we did in the basic tutorial and get the results for training at upper location and testing at lower location.\n\nthe_cross_validator = standard_resample_CV(ds, the_classifier, the_feature_preprocessors);\nthe_cross_validator.num_resample_runs = 10;\nDECODING_RESULTS = the_cross_validator.run_cv_decoding;\n\n\n\nTesting at all locations\nWe will now create a full set of 9 results that are based on training at each of the three locations and testing at each of the three locations. This can be done by creating two loops, one for each training location, and one for each test location, and creating a new datasource each iteration. It should be noted that the generalization_DS is built in such a way that the if a particular original label is mapping into the same training and test class (e.g., if the_training_label_names{1} = {‘car_upper’} and also the_test_label_names{1} = {‘car_upper’}),  the data used in the training set will still come from different trials then the data used for the test set . Thus the cross-validation splits will still be valid because there will not be any of the same data in the training and test sets (if this mapping was done for all classes, one would end up getting the same results as using the basic_DS). Because there is not ‘data leakage’ following code allows for fair comparisons between data that was trained and tested at the same location vs. data that was trained at one location and tested with data from a different location.\n\nmkdir position_invariance_results;  % make a directory to save all the results\nnum_cv_splits = 18;\n\nid_string_names = {'car', 'couch', 'face', 'kiwi', 'flower', 'guitar', 'hand'};\npos_string_names = {'upper', 'middle', 'lower'};\n\nfor iTrainPosition = 1:3\n   for iTestPosition = 1:3\n\n      for iID = 1:7\n            the_training_label_names{iID} = {[id_string_names{iID} '_' pos_string_names{iTrainPosition}]};\n            the_test_label_names{iID} =  {[id_string_names{iID} '_' pos_string_names{iTestPosition}]};\n      end\n\n      ds = generalization_DS(binned_data_file_name, specific_labels_names_to_use, num_cv_splits, the_training_label_names, the_test_label_names);\n       \n      the_cross_validator = standard_resample_CV(ds, the_classifier, the_feature_preprocessors);\n      the_cross_validator.num_resample_runs = 10;\n      DECODING_RESULTS = the_cross_validator.run_cv_decoding;\n\n      save_file_name = ['position_invariance_results/Zhang_Desimone_pos_inv_results_train_pos' num2str(iTrainPosition) '_test_pos' num2str(iTestPosition)]\n\n      save(save_file_name, 'DECODING_RESULTS')\n\n   end\nend\n\n\n\nPlotting the results\nTo plot all these results we will create some custom code. The standard decoding accuracy results are saved in the field DECODING_RESULTS.ZERO_ONE_LOSS_RESULTS.mean_decoding_results. For each training location, we will plot the results of testing at all three locations. We will also add some captions to make the plots easier to read.\n\nposition_names = {'Upper', 'Middle', 'Lower'}\n\nfor iTrainPosition = 1:3\n   \n   for iTestPosition = 1:3\n       load(['position_invariance_results/Zhang_Desimone_pos_inv_results_train_pos' num2str(iTrainPosition) '_test_pos' num2str(iTestPosition)]);\n\n        all_results(iTrainPosition, iTestPosition) = DECODING_RESULTS.ZERO_ONE_LOSS_RESULTS.mean_decoding_results;\n   end\n\n   subplot(1, 3, iTrainPosition)\n   bar(all_results(iTrainPosition, :) .* 100);\n\n   title(['Train ' position_names{iTrainPosition}])\n   ylabel('Classification Accuracy');\n   set(gca, 'XTickLabel', position_names);\n   xlabel('Test position')\n   xLims = get(gca, 'XLim')\n   line([xLims], [1/7 1/7], 'color', [0 0 0]);  % put line at chance decoding accuracy\n\nend\n\nset(gcf, 'position', [250 300 950 300])  % expand the figure\n\nFrom looking at the results we can see that the best decoding accuracies are always obtained when training and testing the classifier at the same location, however the results are well above chance when training the classifier at one location and testing the classifier at a different location showing there is a large degree of position invariance in the Inferior Temporal Cortex.",
    "crumbs": [
      "Tutorials",
      "Generalization analysis"
    ]
  },
  {
    "objectID": "tutorials/meg-eeg-decoding-tutorial/index.html",
    "href": "tutorials/meg-eeg-decoding-tutorial/index.html",
    "title": "MEG/EEG decoding tutorial",
    "section": "",
    "text": "The following tutorial will show you how to perform basic MEG/EEG preprocessing, put the data in raster format, and perform decoding using the MEG/EEG data. This tutorial will use the Isik 26 letter MEG dataset. This tutorial assumes that one is already familiar with the basics of the NDT as covered in the introductory tutorial.\nBefore you begin, you will need to download the additional MEG decoding functions, and add them to your toolbox path.",
    "crumbs": [
      "Tutorials",
      "MEG/EEG decoding tutorial"
    ]
  },
  {
    "objectID": "tutorials/meg-eeg-decoding-tutorial/index.html#about-the-isik-26-letter-dataset",
    "href": "tutorials/meg-eeg-decoding-tutorial/index.html#about-the-isik-26-letter-dataset",
    "title": "MEG/EEG decoding tutorial",
    "section": "About the Isik 26 letter dataset",
    "text": "About the Isik 26 letter dataset\nThe Isik 26 letter MEG dataset was collected by Leyla Isik in Tommy Poggio s lab and the MEG Lab at the McGovern Institute at MIT. The data was used in Figure 2b of the paper: The dynamics of invariant object recognition in the human visual system, J. Neurophys 2014. The data consists of 306 channel (comprised of 102 magentometers, and 204 planar gradiometers) MEG recordings from an Elekta Neuromag Triux Scanner. One subject was shown 26 black, upper-case letters, on a white background, while their neural response was recorded in the MEG scanner. Each letter was presented approximately 50 times. The data is in raster-format, and each trial consists of 233 ms of baseline data where the subject viewed a fixation cross, followed by 50 ms of data when the subject viewed the image of one letter, and 417 ms of data when they again viewed a fixation cross.\nYou can load the MEG data in 2 formats - the raw MEG files output by the scanner (.fif format) and raster format. The raw data download also includes a file with raster labels indicating which stimulus was shown in each trial.\nThe first part of this tutorial outlines how to pre-process this data and convert it to raster format. If you are using the pre-processed raster data you may skip to the decoding section of the tutorial.",
    "crumbs": [
      "Tutorials",
      "MEG/EEG decoding tutorial"
    ]
  },
  {
    "objectID": "tutorials/meg-eeg-decoding-tutorial/index.html#download-brainstorm-and-preprocess-data",
    "href": "tutorials/meg-eeg-decoding-tutorial/index.html#download-brainstorm-and-preprocess-data",
    "title": "MEG/EEG decoding tutorial",
    "section": "Download Brainstorm and preprocess data",
    "text": "Download Brainstorm and preprocess data\nWe will use Brainstorm - a collaborative, open-source application dedicated to MEG/EEG/sEEG/ECoG data analysis - to preprocess our MEG data. If your MEG data is already preprocessed or you prefer a different preprocessing method, you may skip these steps.\nIf you would like to use another MEG data format (besides .fif), please see the Brainstorm website for instructions on how to load it into Brainstorm and complete the preprocessing using the Brainstorm GUI.\nFirst, follow the Brainstorm download and installation instructions.\n\nPreprocess MEG data\nTo preprocess our MEG data, we will first import our MEG events (based on experimental triggers) to Brainstorm and then perform bandpass filtering (0.1-100Hz) to remove external noise. You may follow the Brainstorm tutorial to perform this via their GUI. Or run the following code:\nBefore you run the script, you must open brainstorm (in Matlab), and create a new Brainstorm protocol. To create a new protocol, click on the file menu and select “New protocol”, and name this protocol ‘MEG_decoding_tutorial’.\n\n\n\n\nBrain storm\n\n\n\nNext, you will run the preprocess_with_brainstorm function in Matlab. To do this, specify your RawFilePath (file directory containing your raw MEG .fif files), fileNames (a cell of strings containing the names of your .fif files), and the epochTime (time interval you would like to extract relative to stimulus onset.\n\nRawFilePath = 'Isik_26letter_fif/'; % Change to the appropriate filepath on your computer\nfileNames = {'cbcl000.fif', 'cbcl000-1.fif'};\nepochTime = [-0.2, 0.6]; % -200:600 ms relative to stimulus onset - used in Isik 2014\npreprocess_with_brainstorm(RawFilePath, fileNames, epochTime)\n\n\n\nConvert data to raster format\nBrainstorm saves your imported and pre-processed MEG data in the folder: brainstorm_db/&lt;protocol_name&gt;/data/&lt;subject_name&gt;/&lt;trigger_number&gt;\nWe will next convert the data to raster format. To do this you will need to put your stimulus labels in the structure raster_labels - a structure that contains cell arrays that lists the experimental conditions that were present on each trial (each cell array has as many entries as there are rows in the raster_data matrix, so that there is an experimental condition label for each trial). For example, raster_labels. For example, the variable raster_labels.stimulus_ID contains the labels for which of the 26 stimuli was shown on each trial. The trials in raster_labels should be ordered first by trigger number, and then by stimulus presentation. For example, here we use only one trigger, so the labels are in the order they appeared in the experiment.\nNext, you will run the convert_to_raster function in Matlab. To do this, specify the following variables: brainstorm_db (brainstorm database folder), protocol (protocol name), raster_label_file (name of the file containing your raster_labels), raster_folder (folder to save raster data to), time (length of epochTime indicated above), and triggers (a vector of the trigger IDs used in the experiment).\n(Please note this will take a couple of minutes to run).\n\nprotocol = 'MEG_decoding_tutorial';\nraster_labels_file = 'Isik_26letter_raster_labels.mat';\nraster_folder = 'Isik_26letter_rasters/';\ntime = length(epochTime); %801\ntriggers = 1;\nconvert_to_raster(brainstorm_db, protocol, raster_labels_file, raster_folder, time, triggers)",
    "crumbs": [
      "Tutorials",
      "MEG/EEG decoding tutorial"
    ]
  },
  {
    "objectID": "tutorials/meg-eeg-decoding-tutorial/index.html#decoding-meg-data",
    "href": "tutorials/meg-eeg-decoding-tutorial/index.html#decoding-meg-data",
    "title": "MEG/EEG decoding tutorial",
    "section": "Decoding MEG data",
    "text": "Decoding MEG data\n\nSetting the path and binning the data\nBefore continuing this tutorial, make sure that the path to the NDT has been set as described here. For this tutorial we will use binned-format data that consists of the MEG data in a 50 ms windows. The following code shows how to create this binned data.\n\nraster_folder = 'Isik_26letter_rasters/' % change to your file path\nsave_prefix_name = 'Binned_Isik_26letter_data';\n \nbin_width = 50;\nstep_size = 50;\n\ncreate_binned_data_from_raster_data(raster_folder, save_prefix_name, bin_width, step_size);\n\n\n\nCreating a classifier and a preprocessor\nNext we will create a classifier object and a preprocessor object. We will use the same classifier and preprocessor as was used in the basic tutorial. In addition, we will use an additional feature preprocessor, the select_or_exclude_top_k_features_FP.  The MEG data contains recordings from 306 channels, many of which do not contain visual information and are thus helpful to remove. This preprocessor selects the top k most informative channels based on a ANOVA on the training data. For this analysis we will choose the top 25 features.\n\nthe_classifier = max_correlation_coefficient_CL;\nthe_feature_preprocessors{1} = zscore_normalize_FP;\nthe_feature_preprocessors{2} = select_or_exclude_top_k_features_FP;\nthe_feature_preprocessors{2}.num_features_to_use = 25;\n\n\n\nAveraging stimulus repetitions\nThe strength of our decoding signal can be greatly improved if we average trials where the same stimulus was repeated. avg_DS implements the functions of the basic_DS with the additional feature that it can average together trials in a given cross validation split. We will use 5 cross validation splits with 10 trials each, and average together all 10 trials in each cross validation split. In addition, since the MEG data was collected simultaneously we will set the create_simultaneously_recorded_populations flag to 1.\n\nnum_cv_splits = 5;\nbinned_data_file_name = 'Binned_Isik_26letter_data_50ms_bins_50ms_sampled.mat';\nspecific_labels_names_to_use = 'stim_ID';\nnAvg = 10;\nds = avg_DS(binned_data_file_name, specific_labels_names_to_use, num_cv_splits, nAvg);\nds.num_times_to_repeat_each_label_per_cv_split = 10;\nds.create_simultaneously_recorded_populations = 1;\n\nWe can then create a cross-validator as we did in the basic tutorial, and save the decoding results.\n\nthe_cross_validator = standard_resample_CV(ds, the_classifier, the_feature_preprocessors);\nthe_cross_validator.num_resample_runs = 10;\nDECODING_RESULTS = the_cross_validator.run_cv_decoding;\n\nsave_file_name = ['Isik_26letter_results_' num2str(bin_width) 'msbins_' num2str(step_size) 'mssteps'];\nsave(save_file_name, 'DECODING_RESULTS')\n\n\n\nPlotting the results\nTo plot all these results, we will use the same code as the standard tutorial. The standard decoding accuracy results are saved in the field DECODING_RESULTS.ZERO_ONE_LOSS_RESULTS.mean_decoding_results. Below we show how to plot the results we created above setting only a few of the possible parameters.\n\nresult_names{1} = save_file_name;  \n \n% create the plot results object\nplot_obj = plot_standard_results_object(result_names);\n \n% put a line at the time when the stimulus was shown\nplot_obj.significant_event_times = 233; \n \n% display the results\nplot_obj.plot_results;\n\nFrom looking at the results we can see that we can decode which letter the subject was viewing around 70 ms after image onset.\n\n\nConclusions\nWe recommend trying out this tutorial yourself in Matlab and experimenting with different datasource, feature-preprocessor, cross-validator and plotting parameters. Once you feel comfortable with this tutorial you can look at the generalization analysis tutorial which shows how to test whether neural representations contain information in an abstract/invariant format, or you can look at the  getting started with your own data tutorial which shows the steps necessary to analyze your own data.\nYou can also run generalization analyses with your own MEG data. To get started, refer to the steps in the Generalization analysis tutorial and the avg_generalization_DS, which allows you to average trials together in the same manner as the avg_DS.\nThis tutorial was created by Leyla Isik",
    "crumbs": [
      "Tutorials",
      "MEG/EEG decoding tutorial"
    ]
  },
  {
    "objectID": "downloads/datasets/qi-constantinidis-pre-and-post-training-dataset/index.html",
    "href": "downloads/datasets/qi-constantinidis-pre-and-post-training-dataset/index.html",
    "title": "Qi Constantinidis pre/post training dataset",
    "section": "",
    "text": "The Qi-Constantinidis pre and post training dataset was collected by Xue-Lian Qi in Christos Constantinidis lab. The data consists of single unit recordings from the prefrontal cortex (PFC) while monkeys passively viewed images (in a feature and spatial configurations), and also after monkey were trained to engaged in a delayed match-to-sample task with these images.\n\nIf you plan to use this data you must cite the publications below. More information about the experiments and data analyses can be found in these papers as well.\nMeyers E, Qi XL, Constantinidis C (2012). Incorporation of new information into prefrontal cortical activity after learning working memory tasks. Proceedings of the National Academy of Sciences, 109:4651-4656.\nQi X-L, Meyer T, Stanford TR, Constantinidis C (2011).  Changes in prefrontal neuronal activity after learning to perform a spatial working memory task. Cereb Cortex 21:2722-2732\n\nDownload Qi-Constantinidis pre/post training data",
    "crumbs": [
      "Downloads",
      "Datasets",
      "Qi Constantinidis pre/post training dataset"
    ]
  },
  {
    "objectID": "downloads/datasets/freiwald-tsao-face-views-am-dataset/index.html",
    "href": "downloads/datasets/freiwald-tsao-face-views-am-dataset/index.html",
    "title": "Freiwald Tsao FaceViews AM",
    "section": "",
    "text": "The Freiwald Tsao Face Views AM dataset was collected by Winrich Freiwald (Rockefeller University) and Doris Tsao (UC Berkeley), and was originally published in Freiwald and Taso (2010) . The data consists of single unit recordings from anterior medial face patch (AM) while monkeys passively viewed images of 25 individuals from 8 different head orientations. A decoding analysis of this data was published in Meyers et al (2015).\n\n\n\n\n\nIf you plan to use this data you must cite the publications below. More information about the experiments and data analyses can be found in these papers as well.\n\n\nFreiwald, W. A., & Tsao, D. Y. (2010). Functional compartmentalization and viewpoint generalization within the macaque face-processing system. Science, 330(6005), 845-851.\n\n\nMeyers, E. M., Borzello, M., Freiwald, W. A., & Tsao, D. (2015). Intelligent information loss: The coding of facial identity, head pose, and non-face information in the macaque face patch system. Journal of Neuroscience, 35(18).\n\n\nDownload MATLAB version of the Freiwald Tsao Face Views AM dataset\nDownload csv version of the Freiwald Tsao Face Views AM dataset\nDownload R version of the Freiwald Tsao Face Views AM dataset",
    "crumbs": [
      "Downloads",
      "Datasets",
      "Freiwald Tsao FaceViews AM"
    ]
  },
  {
    "objectID": "downloads/index.html",
    "href": "downloads/index.html",
    "title": "Downloads",
    "section": "",
    "text": "Download the Neural Decoding Toolbox\nDownload example datasets in raster-format\nDownload a version of LibSVM that will work with the Neural Decoding Toolbox",
    "crumbs": [
      "Downloads"
    ]
  },
  {
    "objectID": "downloads/download-the-toolbox/index.html",
    "href": "downloads/download-the-toolbox/index.html",
    "title": "Download the toolbox",
    "section": "",
    "text": "Download the latest version of the Neural Decoding Toolbox (v1.0.4)\n\n\n\nThe latest version of additional files that are useful for MEG decoding can be downloaded here\n\n\nOlder versions of the toolbox can be downloaded here. In general we highly recommend using the latest version of the toolbox. However in order to replicate previous results, we maintain links to older versions of the toolbox.",
    "crumbs": [
      "Downloads",
      "Download the toolbox"
    ]
  },
  {
    "objectID": "downloads/download-the-toolbox/lastest_version/index.html",
    "href": "downloads/download-the-toolbox/lastest_version/index.html",
    "title": "Lastest toolbox version",
    "section": "",
    "text": "Download the latest version of the Neural Decoding Toolbox (v1.0.4)\n\n\n\nThe latest version of additional files that are useful for MEG decoding can be downloaded here",
    "crumbs": [
      "Downloads",
      "Download the toolbox",
      "Lastest toolbox version"
    ]
  },
  {
    "objectID": "downloads/additional-code/index.html",
    "href": "downloads/additional-code/index.html",
    "title": "Additional code",
    "section": "",
    "text": "Below are links to additional code that works with the Neural Decoding Toolbox:\n\n\nLIBSVM\n\n\n MEG decoding additional functions",
    "crumbs": [
      "Downloads",
      "Additional code"
    ]
  }
]